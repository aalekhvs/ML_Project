{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvHV1MbnYeZg"
   },
   "source": [
    "# Assistive Keyboard — 7B (Colab A100, Drive‑persistent)\n",
    "\n",
    "- base: `Qwen/Qwen2.5-7B-Instruct`\n",
    "- Drive-backed code/data/adapters/results\n",
    "- per-user LoRA + lexicon + RAG; eval KSS/latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tM-u6ik7YeZg",
    "outputId": "bbf435fd-3649-4ce8-dfc4-e54778da655b"
   },
   "outputs": [],
   "source": [
    "# gpu sanity\n",
    "import sys, torch, platform\n",
    "print('py:', sys.version.split()[0], '| cuda:', torch.cuda.is_available(), '| plat:', platform.platform())\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEK4kHvmYeZh",
    "outputId": "04986426-50b0-42ca-c81a-5729f7d32e55"
   },
   "outputs": [],
   "source": [
    "# mount drive + dirs\n",
    "from google.colab import drive; drive.mount('/content/drive')\n",
    "from pathlib import Path; import os\n",
    "PROJ = Path('/content/drive/MyDrive/assistive_keyboard_7B'); PROJ.mkdir(parents=True, exist_ok=True)\n",
    "CODE = PROJ/'code'; CODE.mkdir(exist_ok=True)\n",
    "DATA = PROJ/'data'; (DATA/'processed').mkdir(parents=True, exist_ok=True)\n",
    "SPLITS = PROJ/'splits'; SPLITS.mkdir(exist_ok=True)\n",
    "USERS = PROJ/'users'; USERS.mkdir(exist_ok=True)\n",
    "LEX = PROJ/'lexicons'; LEX.mkdir(exist_ok=True)\n",
    "RAGD = PROJ/'rag'; RAGD.mkdir(exist_ok=True)\n",
    "ADAPT = PROJ/'adapters'; ADAPT.mkdir(exist_ok=True)\n",
    "RUNS = PROJ/'runs'; RUNS.mkdir(exist_ok=True)\n",
    "CACHE = PROJ/'hf_cache'; CACHE.mkdir(exist_ok=True)\n",
    "os.environ['HF_HOME'] = str(CACHE)\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(CACHE)\n",
    "print('root:', PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXjRw8o9YeZh"
   },
   "outputs": [],
   "source": [
    "# config (demo knobs)\n",
    "MAX_TEST_AUTHORS = 20\n",
    "ADAPT_TOKENS     = 2000\n",
    "VAL_TOKENS       = 800\n",
    "TEST_TOKENS      = 2000\n",
    "LORA_STEPS       = 600\n",
    "LORA_RANK        = 8\n",
    "LORA_ALPHA       = 16\n",
    "LORA_DROPOUT     = 0.05\n",
    "BASE_MODEL       = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "USE_BF16_INSTEAD_OF_4BIT = False  # flip if 4bit acts up; A100-80G can do bf16 easily\n",
    "SEED = 42\n",
    "\n",
    "# seeds\n",
    "import os, random, numpy as np, torch\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esViFFgsYeZh",
    "outputId": "dffd2137-5bac-4d85-ce28-d763dc14b23a"
   },
   "outputs": [],
   "source": [
    "# deps\n",
    "%%bash\n",
    "set -e\n",
    "pip -q install --upgrade pip\n",
    "pip -q install numpy pandas tqdm pyyaml regex scikit-learn ujson\n",
    "pip -q install transformers accelerate datasets sentence-transformers\n",
    "pip -q install faiss-cpu peft bitsandbytes bert-score mauve-text\n",
    "python - <<'PY'\n",
    "import torch; print('torch', torch.__version__, 'cuda?', torch.cuda.is_available())\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCmjr3HlYeZh",
    "outputId": "f4344936-c4b9-4a22-f9c6-ee7e396525c7"
   },
   "outputs": [],
   "source": [
    "# project code → Drive\n",
    "from pathlib import Path; import textwrap\n",
    "for sub in ['src/utils','src/data','src/splits','src/lexicon','src/rag','src/lora','src/infer','src/eval']:\n",
    "    (CODE/sub).mkdir(parents=True, exist_ok=True)\n",
    "(CODE/'src/__init__.py').write_text('')\n",
    "(CODE/'src/utils/__init__.py').write_text('')\n",
    "(CODE/'src/utils/io.py').write_text(textwrap.dedent('''\n",
    "from pathlib import Path\n",
    "import json, ujson\n",
    "def read_lines(p):\n",
    "    return Path(p).read_text(encoding='utf-8').splitlines()\n",
    "def write_lines(p, lines):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(p).write_text('\\n'.join(lines), encoding='utf-8')\n",
    "def read_jsonl(p):\n",
    "    out=[]\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line: out.append(json.loads(line))\n",
    "    return out\n",
    "def write_jsonl(p, rows):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p,'w',encoding='utf-8') as f:\n",
    "        for r in rows: f.write(ujson.dumps(r, ensure_ascii=False)+'\\n')\n",
    "'''))\n",
    "(CODE/'src/data/__init__.py').write_text('')\n",
    "(CODE/'src/data/clean.py').write_text(textwrap.dedent(r'''\n",
    "import re\n",
    "QUOTE_RE = re.compile(r'(?m)^(>+).*?$')\n",
    "SIG_RE = re.compile(r'(?ims)--\\s*\\n.*?$')\n",
    "def clean_text(s:str)->str:\n",
    "    s=s.replace('\\r\\n','\\n')\n",
    "    s=re.sub(QUOTE_RE,'',s)\n",
    "    s=re.sub(SIG_RE,'',s)\n",
    "    s=re.sub(r'[ \\t]+',' ',s)\n",
    "    s=re.sub(r'\\n{3,}','\\n\\n',s)\n",
    "    return s.strip()\n",
    "def approx_token_count(s:str)->int:\n",
    "    return len(re.findall(r\"\\w+|[.,!?;:]\", s))\n",
    "'''))\n",
    "(CODE/'src/data/enron_loader.py').write_text(textwrap.dedent(r'''\n",
    "from pathlib import Path\n",
    "from .clean import clean_text, approx_token_count\n",
    "from src.utils.io import write_jsonl\n",
    "def build_authors_jsonl(maildir_root: str, out_jsonl: str, min_doc_tokens: int = 20):\n",
    "    rows=[]; maildir=Path(maildir_root)\n",
    "    for user_dir in maildir.iterdir():\n",
    "        if not user_dir.is_dir(): continue\n",
    "        author_id=user_dir.name\n",
    "        for p in user_dir.rglob('*'):\n",
    "            if not p.is_file(): continue\n",
    "            try: txt=p.read_text(errors='ignore')\n",
    "            except Exception: continue\n",
    "            txt=clean_text(txt)\n",
    "            if approx_token_count(txt)>=min_doc_tokens:\n",
    "                rows.append({'author_id':author_id,'doc_id':str(p.relative_to(maildir)),'text':txt})\n",
    "    write_jsonl(out_jsonl, rows); print(f'wrote {len(rows)} docs → {out_jsonl}')\n",
    "'''))\n",
    "(CODE/'src/splits/__init__.py').write_text('')\n",
    "(CODE/'src/splits/make_splits.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from src.utils.io import read_jsonl, write_lines\n",
    "from src.data.clean import approx_token_count\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--authors_jsonl', required=True)\n",
    "    ap.add_argument('--out_dir', required=True)\n",
    "    ap.add_argument('--min_tokens', type=int, default=4000)\n",
    "    ap.add_argument('--adapt_tokens', type=int, default=2000)\n",
    "    ap.add_argument('--val_tokens', type=int, default=800)\n",
    "    ap.add_argument('--test_tokens', type=int, default=2000)\n",
    "    ap.add_argument('--max_test_authors', type=int, default=4)\n",
    "    ap.add_argument('--seed', type=int, default=42)\n",
    "    args=ap.parse_args(); random.seed(args.seed)\n",
    "    out_dir=Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows=read_jsonl(args.authors_jsonl)\n",
    "    by_author=defaultdict(list)\n",
    "    for r in rows: by_author[r['author_id']].append(r['text'])\n",
    "    kept={}\n",
    "    for a,docs in by_author.items():\n",
    "        tot=sum(approx_token_count(t) for t in docs)\n",
    "        if tot>=args.min_tokens: kept[a]=docs\n",
    "    authors=sorted(kept.keys()); random.shuffle(authors)\n",
    "    n=len(authors); n_train=int(0.70*n); n_dev=int(0.15*n)\n",
    "    train_ids=authors[:n_train]; dev_ids=authors[n_train:n_train+n_dev]; test_ids=authors[n_train+n_dev:]\n",
    "    test_ids=test_ids[:args.max_test_authors]\n",
    "    write_lines(out_dir/'authors_train.txt', train_ids)\n",
    "    write_lines(out_dir/'authors_dev.txt', dev_ids)\n",
    "    write_lines(out_dir/'authors_test.txt', test_ids)\n",
    "    users_dir=Path(str(Path(out_dir).parent/'users')); users_dir.mkdir(exist_ok=True)\n",
    "    for a in test_ids:\n",
    "        texts=kept[a][:]; random.shuffle(texts)\n",
    "        acc=0; adapt=[]; val=[]; test=[]\n",
    "        for t in texts:\n",
    "            tc=approx_token_count(t)\n",
    "            if acc<args.adapt_tokens: adapt.append(t); acc+=tc\n",
    "            elif acc<args.adapt_tokens+args.val_tokens: val.append(t); acc+=tc\n",
    "            else: test.append(t)\n",
    "        udir=users_dir/a; udir.mkdir(parents=True, exist_ok=True)\n",
    "        (udir/'adapt.txt').write_text('\\n\\n'.join(adapt), encoding='utf-8')\n",
    "        (udir/'val.txt').write_text('\\n\\n'.join(val), encoding='utf-8')\n",
    "        (udir/'test.txt').write_text('\\n\\n'.join(test), encoding='utf-8')\n",
    "    print(f'train/dev/test: {len(train_ids)}/{len(dev_ids)}/{len(test_ids)} | users/* ready')\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/lexicon/__init__.py').write_text('')\n",
    "(CODE/'src/lexicon/build_lexicon.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, json, re, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tok(s):\n",
    "    return re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\sA-Za-z0-9]\", s)\n",
    "def build_lex(text, k=4000):\n",
    "    v=TfidfVectorizer(tokenizer=tok, lowercase=True, ngram_range=(1,2), min_df=2, max_df=0.9, use_idf=True, smooth_idf=True, norm=None)\n",
    "    X=v.fit_transform(text.splitlines()); vocab=v.get_feature_names_out()\n",
    "    scores=np.asarray(X.sum(axis=0)).ravel(); idx=scores.argsort()[::-1]\n",
    "    top=[(vocab[i], float(scores[i])) for i in idx[:k]]\n",
    "    return {'entries':[{'token':t,'score':s} for t,s in top]}\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser(); ap.add_argument('--users_dir', required=True); ap.add_argument('--out_dir', required=True); ap.add_argument('--max_items', type=int, default=4000); args=ap.parse_args()\n",
    "    out=Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        p=u/'adapt.txt'\n",
    "        if not p.exists(): continue\n",
    "        text=p.read_text(encoding='utf-8'); lex=build_lex(text, args.max_items)\n",
    "        (out/f'{u.name}.lexicon.json').write_text(json.dumps(lex, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "        print('lex:', u.name)\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/rag/__init__.py').write_text('')\n",
    "(CODE/'src/rag/build_rag.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "def chunks(s, m=300, ov=50):\n",
    "    s=s.strip(); out=[]; i=0\n",
    "    while i<len(s): out.append(s[i:i+m]); i+=max(1, m-ov)\n",
    "    return out\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser(); ap.add_argument('--users_dir', required=True); ap.add_argument('--out_dir', required=True)\n",
    "    ap.add_argument('--model_name', default='sentence-transformers/all-MiniLM-L6-v2'); ap.add_argument('--chunk_chars', type=int, default=300); ap.add_argument('--overlap', type=int, default=50)\n",
    "    args=ap.parse_args(); emb=SentenceTransformer(args.model_name)\n",
    "    out=Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        p=u/'adapt.txt'\n",
    "        if not p.exists(): continue\n",
    "        cs=chunks(p.read_text(encoding='utf-8'), args.chunk_chars, args.overlap)\n",
    "        if not cs: continue\n",
    "        X=emb.encode(cs, batch_size=64, convert_to_numpy=True, show_progress_bar=False).astype(np.float32)\n",
    "        faiss.normalize_L2(X); idx=faiss.IndexFlatIP(X.shape[1]); idx.add(X)\n",
    "        faiss.write_index(idx, str(out/f'{u.name}.faiss'))\n",
    "        (out/f'{u.name}.chunks.json').write_text(json.dumps(cs, ensure_ascii=False), encoding='utf-8')\n",
    "        print('rag:', u.name, len(cs))\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/lora/__init__.py').write_text('')\n",
    "(CODE/'src/lora/train_lora.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, re\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "def read_txt(p): return Path(p).read_text(encoding='utf-8')\n",
    "def mk_ds(txt: str, tok, bs=256):\n",
    "    ids=tok(txt, return_tensors=None, truncation=False)['input_ids']\n",
    "    blocks=[ids[i:i+bs] for i in range(0, len(ids)-bs, bs)] or [ids[:bs]]\n",
    "    return Dataset.from_dict({'input_ids': blocks})\n",
    "def last_ck(d: Path):\n",
    "    c=[p for p in d.glob('checkpoint-*') if p.is_dir()]\n",
    "    if not c: return None\n",
    "    def step(p):\n",
    "        import re; m=re.search(r'checkpoint-(\\d+)', p.name); return int(m.group(1)) if m else -1\n",
    "    return sorted(c, key=step)[-1]\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--users_dir', required=True)\n",
    "    ap.add_argument('--adapters_dir', required=True)\n",
    "    ap.add_argument('--base_model', default='Qwen/Qwen2.5-7B-Instruct')\n",
    "    ap.add_argument('--rank', type=int, default=8)\n",
    "    ap.add_argument('--alpha', type=int, default=16)\n",
    "    ap.add_argument('--dropout', type=float, default=0.05)\n",
    "    ap.add_argument('--lr', type=float, default=2e-4)\n",
    "    ap.add_argument('--steps', type=int, default=300)\n",
    "    ap.add_argument('--block_size', type=int, default=256)\n",
    "    args=ap.parse_args()\n",
    "    tok=AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
    "    model=AutoModelForCausalLM.from_pretrained(args.base_model, device_map='auto', load_in_4bit=True, torch_dtype=torch.float16)\n",
    "    model=prepare_model_for_kbit_training(model)\n",
    "    model=get_peft_model(model, LoraConfig(r=args.rank, lora_alpha=args.alpha, lora_dropout=args.dropout, bias='none', task_type='CAUSAL_LM'))\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        a=u/'adapt.txt'; v=u/'val.txt'\n",
    "        if not a.exists() or not v.exists(): continue\n",
    "        out=Path(args.adapters_dir)/u.name; out.mkdir(parents=True, exist_ok=True)\n",
    "        tr=mk_ds(read_txt(a), tok, args.block_size); dv=mk_ds(read_txt(v), tok, args.block_size)\n",
    "        targs=TrainingArguments(output_dir=str(out), per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8,\n",
    "            logging_steps=10, learning_rate=args.lr, max_steps=args.steps, evaluation_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=3, report_to='none')\n",
    "        def collate(batch):\n",
    "            feats=[b['input_ids'] for b in (batch if isinstance(batch,list) else [batch])]\n",
    "            ml=max(len(f) for f in feats); pad=tok.pad_token_id\n",
    "            ids=[f + [pad]*(ml-len(f)) for f in feats]\n",
    "            return {'input_ids': torch.tensor(ids), 'labels': torch.tensor(ids)}\n",
    "        ck=last_ck(out)\n",
    "        Trainer(model=model, args=targs, train_dataset=tr, eval_dataset=dv, data_collator=collate).train(resume_from_checkpoint=str(ck) if ck else None)\n",
    "        model.save_pretrained(str(out/'lora_adapter'))\n",
    "        print('adapter:', out/'lora_adapter')\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/infer/__init__.py').write_text('')\n",
    "(CODE/'src/infer/suggest.py').write_text(textwrap.dedent(r'''\n",
    "from collections import defaultdict, Counter\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "import numpy as np, torch\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def tok_basic(s): return re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\sA-Za-z0-9]\", s)\n",
    "class NGram:\n",
    "    def __init__(self, text: str, n: int = 3):\n",
    "        toks=tok_basic(text.lower()); self.n=n; self.ng=defaultdict(Counter)\n",
    "        for i in range(len(toks)-n): self.ng[tuple(toks[i:i+n-1])][toks[i+n-1]]+=1\n",
    "    def suggest(self, ctx: str, k: int = 3):\n",
    "        toks=tok_basic(ctx.lower()); key=tuple(toks[-(self.n-1):]) if len(toks)>=self.n-1 else tuple(toks)\n",
    "        cand=self.ng.get(key,{}); return [w for w,_ in cand.most_common(k)]\n",
    "class Bias(LogitsProcessor):\n",
    "    def __init__(self, mp): self.mp=mp or {}\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        if self.mp: scores[:, list(self.mp.keys())]+=torch.tensor(list(self.mp.values()), device=scores.device)\n",
    "        return scores\n",
    "class Lex:\n",
    "    def __init__(self, tok, lex_json, cap=2.5):\n",
    "        self.tok=tok; self.mp={}\n",
    "        try: entries=json.loads(lex_json)['entries']\n",
    "        except Exception: entries=[]\n",
    "        for e in entries[:2000]:\n",
    "            ids=self.tok(e['token'], add_special_tokens=False)['input_ids']\n",
    "            if len(ids)==1: self.mp[ids[0]]=cap\n",
    "    def proc(self): return Bias(self.mp)\n",
    "class RAG:\n",
    "    def __init__(self, f, c, em='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.idx=faiss.read_index(str(f)); self.ch=json.loads(Path(c).read_text(encoding='utf-8'))\n",
    "        self.emb=SentenceTransformer(em)\n",
    "    def top(self, text, k=4):\n",
    "        q=self.emb.encode([text], convert_to_numpy=True).astype(np.float32); faiss.normalize_L2(q); D,I=self.idx.search(q,k)\n",
    "        return [self.ch[i] for i in I[0] if i>=0]\n",
    "class LLM:\n",
    "    def __init__(self, base: str, adapter_dir: str=None, rag=None, lex=None, max_ctx=512, bf16=False):\n",
    "        self.tok=AutoTokenizer.from_pretrained(base, use_fast=True)\n",
    "        if bf16:\n",
    "            self.m=AutoModelForCausalLM.from_pretrained(base, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "        else:\n",
    "            self.m=AutoModelForCausalLM.from_pretrained(base, device_map='auto', load_in_4bit=True)\n",
    "        if adapter_dir and Path(adapter_dir).exists(): self.m=PeftModel.from_pretrained(self.m, adapter_dir)\n",
    "        self.rag=rag; self.lex=lex; self.max_ctx=max_ctx; self.m.eval()\n",
    "    def _prompt(self, tail, mem):\n",
    "        if mem:\n",
    "            bullets='\\n'.join(f'- {c[:200]}' for c in mem)\n",
    "            mem=f\"Memory\\n{bullets}\\n\\n\"\n",
    "        return f\"Continue in the user's style.\\n{mem}Draft:\\n{tail}\\n\\nContinue:\"\n",
    "    def suggest(self, ctx: str, k: int = 3):\n",
    "        tail=ctx[-1000:]; mem=self.rag.top(tail,4) if self.rag else []\n",
    "        prompt=self._prompt(tail, mem)\n",
    "        ids=self.tok(prompt, return_tensors='pt', truncation=True, max_length=self.max_ctx).to(self.m.device)\n",
    "        procs=[self.lex.proc()] if self.lex else None\n",
    "        with torch.no_grad():\n",
    "            out=self.m.generate(**ids, max_new_tokens=6, do_sample=False, num_beams=max(1,k), num_return_sequences=k,\n",
    "                                 logits_processor=procs, pad_token_id=self.tok.eos_token_id)\n",
    "        texts=self.tok.batch_decode(out[:, ids['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        res=[]\n",
    "        for t in texts:\n",
    "            t=t.strip(); m=re.match(r\"^\\S{1,8}\", t); s=m.group(0) if m else t[:8]\n",
    "            if s and s not in res: res.append(s)\n",
    "        return res[:k]\n",
    "'''))\n",
    "(CODE/'src/eval/__init__.py').write_text('')\n",
    "(CODE/'src/eval/typing_sim.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, time, csv\n",
    "from pathlib import Path\n",
    "from src.infer.suggest import NGram, LLM, Lex, RAG\n",
    "def load_text(p): return Path(p).read_text(encoding='utf-8')\n",
    "def sim(doc: str, sugg, k=3, max_chunk=8):\n",
    "    kp=0; kw=0; acc=0; t0=time.time(); i=0\n",
    "    while i<len(doc):\n",
    "        kp+=1; kw+=1; i+=1\n",
    "        pref=doc[:i]; sug=sugg.suggest(pref, k=k)\n",
    "        if not sug: continue\n",
    "        remain=doc[i:]; ok=False\n",
    "        for s in sug:\n",
    "            s=s[:max_chunk]\n",
    "            if remain.lower().startswith(s.lower()):\n",
    "                saved=max(len(s)-1,0); kw+=1; kw-=saved; i+=len(s); acc+=1; ok=True; break\n",
    "        if not ok: continue\n",
    "    ms=(time.time()-t0)*1000.0; kss=1.0-(kw/max(kp,1))\n",
    "    return dict(keys_plain=kp, keys_with=kw, kss=kss, accepts=acc, time_ms=ms)\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--users_dir', required=True)\n",
    "    ap.add_argument('--mode', choices=['ngram','llm_base','llm_lex','llm_full'], default='ngram')\n",
    "    ap.add_argument('--base_model', default='Qwen/Qwen2.5-7B-Instruct')\n",
    "    ap.add_argument('--adapters_dir', default='adapters')\n",
    "    ap.add_argument('--lexicons_dir', default='lexicons')\n",
    "    ap.add_argument('--rag_dir', default='rag')\n",
    "    ap.add_argument('--results_csv', required=True)\n",
    "    ap.add_argument('--k', type=int, default=3)\n",
    "    ap.add_argument('--bf16', action='store_true')\n",
    "    args=ap.parse_args(); out=[]\n",
    "    users=Path(args.users_dir)\n",
    "    for u in users.iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        a=u/'adapt.txt'; t=u/'test.txt'\n",
    "        if not a.exists() or not t.exists(): continue\n",
    "        if args.mode=='ngram':\n",
    "            sg=NGram(load_text(a))\n",
    "        else:\n",
    "            from transformers import AutoTokenizer\n",
    "            lex=None\n",
    "            if args.mode in ('llm_lex','llm_full'):\n",
    "                lp=Path(args.lexicons_dir)/f'{u.name}.lexicon.json'\n",
    "                if lp.exists(): lex=Lex(AutoTokenizer.from_pretrained(args.base_model, use_fast=True), lp.read_text(encoding='utf-8'), cap=2.5)\n",
    "            rag=None\n",
    "            fp=Path(args.rag_dir)/f'{u.name}.faiss'; cp=Path(args.rag_dir)/f'{u.name}.chunks.json'\n",
    "            if fp.exists() and cp.exists(): rag=RAG(fp, cp)\n",
    "            ad=str(Path(args.adapters_dir)/u.name/'lora_adapter') if args.mode=='llm_full' else None\n",
    "            sg=LLM(args.base_model, adapter_dir=ad, rag=rag, lex=lex, bf16=args.bf16)\n",
    "        res=sim(load_text(t), sg, k=args.k)\n",
    "        out.append({'user': u.name, 'mode': args.mode, **res}); print(u.name, args.mode, f\"KSS={res['kss']:.3f}\")\n",
    "    Path(args.results_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(args.results_csv,'w',newline='',encoding='utf-8') as f:\n",
    "        w=csv.DictWriter(f, fieldnames=list(out[0].keys()) if out else ['user','mode','kss'])\n",
    "        w.writeheader(); [w.writerow(r) for r in out]\n",
    "    print('csv →', args.results_csv)\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "print('code rooted at', CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKnVIYDKYeZh"
   },
   "source": [
    "### data → authors.jsonl (robust download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_U21MAyb5u1",
    "outputId": "a0126f2c-d379-46d1-fe2e-5a9fc4e22c89"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# fix the broken io.py\n",
    "python - <<'PY'\n",
    "from pathlib import Path\n",
    "p = Path('/content/drive/MyDrive/assistive_keyboard_7B/code/src/utils/io.py')\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "p.write_text(\n",
    "\"\"\"from pathlib import Path\n",
    "import json, ujson\n",
    "\n",
    "def read_lines(p):\n",
    "    return Path(p).read_text(encoding='utf-8').splitlines()\n",
    "\n",
    "def write_lines(p, lines):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(p).write_text('\\\\n'.join(lines), encoding='utf-8')\n",
    "\n",
    "def read_jsonl(p):\n",
    "    out=[]\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def write_jsonl(p, rows):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p,'w',encoding='utf-8') as f:\n",
    "        for r in rows:\n",
    "            f.write(ujson.dumps(r, ensure_ascii=False)+'\\\\n')\n",
    "\"\"\"\n",
    ", encoding='utf-8')\n",
    "print(\"fixed:\", p)\n",
    "PY\n",
    "\n",
    "# now build authors.jsonl from the extracted 20110402 maildir\n",
    "python - <<'PY'\n",
    "from pathlib import Path, sys\n",
    "sys.path.append('/content/drive/MyDrive/assistive_keyboard_7B/code')\n",
    "from src.data.enron_loader import build_authors_jsonl\n",
    "maildir = '/content/enron_mail_20110402/maildir'\n",
    "out = '/content/drive/MyDrive/assistive_keyboard_7B/data/processed/authors.jsonl'\n",
    "Path(out).parent.mkdir(parents=True, exist_ok=True)\n",
    "build_authors_jsonl(maildir, out, min_doc_tokens=20)\n",
    "print('authors.jsonl ✓ ->', out)\n",
    "PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJZFG7qvYeZh"
   },
   "source": [
    "### splits (author‑disjoint) + per‑user slices (use config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WglevcDrYeZi",
    "outputId": "9aec9c33-dde6-488f-ba25-f50109a5ebfb"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path; import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B')\n",
    "sys.path.append(str(root/'code'))\n",
    "sys.argv = [\n",
    "  'splits',\n",
    "  '--authors_jsonl', str(root/'data/processed/authors.jsonl'),\n",
    "  '--out_dir',       str(root/'splits'),\n",
    "  '--min_tokens',    str(ADAPT_TOKENS+VAL_TOKENS+TEST_TOKENS),\n",
    "  '--adapt_tokens',  str(ADAPT_TOKENS),\n",
    "  '--val_tokens',    str(VAL_TOKENS),\n",
    "  '--test_tokens',   str(TEST_TOKENS),\n",
    "  '--max_test_authors', str(MAX_TEST_AUTHORS),\n",
    "  '--seed',          str(SEED)\n",
    "]\n",
    "from src.splits.make_splits import main as run; run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAhAhDEqYeZi"
   },
   "source": [
    "### prefetch base model (warms HF cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632,
     "referenced_widgets": [
      "b4837667578a47c5b44038f8b6fe07b6",
      "faf1b7fae0a54601b526e6fec72a1084",
      "dcb90c2456124592b224d73d0a4bd1cf",
      "2757a5b14ba04c389ae5abb0930e6bd0",
      "cbd53e8e2ea846ca8685fe59f9acab63",
      "0da5201c907148fe941d095ecc79ea4a",
      "555287ab105b4d8f92fc5b313b88e1bb",
      "d3710d12561d4ff0b4a3b4f66151593c",
      "b87fb8c6ebda45768c3e483d7b1f7fc1",
      "3c1f2e0d330845c0a29d8659de818a17",
      "0836f4f1c15c42f4b0804e87432d79ce",
      "e07e6954d147409899fd16792c002957",
      "66b41f9b12dc4c9e90827d3718998c36",
      "ec6db34692454a3982ccc569346cec2e",
      "9bae3c440f774f5bb796cfb236a608f4",
      "b874e760207a4fce9cd69689286b6713",
      "be108b685d76445781084d20b24dd37c",
      "dd14b45b330447cf8db0149622b7e353",
      "57071863f82b4014baec1db64e05da9d",
      "1717360905d9487db040a9fe63f4d013",
      "301e6eccfea84200ba88e65d0153c73a",
      "397e18e2daf947318b59227a5448d499",
      "ca11c5119b7349ed8f26586096af1ef5",
      "ae23c5e4e6bc4477a011d2408f2da68e",
      "000bb552c586431d82bc05f5ef5e102c",
      "351d9f5943fe4154aa6699a103640098",
      "98e8ab340eca4b4daf2f672dd41d3438",
      "905356691c9e4c95bf51e979fb6021be",
      "c0a054b5561146949b490445051d528b",
      "983ac10535744a3caf31a5416fe6e67d",
      "d7575cc21b1a4e85b2f852c73061597d",
      "009a46213c4f463ca41b23c226cf6ffe",
      "2513db6e5433424cbc679a9065869e78",
      "cc3fd75cc89a4b60800410a8f738ebba",
      "03b58487088b48a081cbb064418b736c",
      "8d68c53b2302406bb56e0e0f781b88cb",
      "82f04d25878a4837a581aaf3c6828788",
      "a101330305cc4aa889e296e17a7b0292",
      "673b4b2873fd42929d02b89dac825dac",
      "b2428459e5a34cb684c72e351fa60320",
      "f3aeea46a769479ca7efbaa37072c23c",
      "26f79a2ff9054937b0b5f078d86ed29c",
      "ce44edd840014ec6a58151df6c961b14",
      "acc496f7c7ad46c8a1c27f4714df5185",
      "b51dd153f6594045b1bb0e2dda06be02",
      "1fe489b750084b4c95a903751d8507fc",
      "b987655b30b643438431956b81e3d47d",
      "aa79955e9f6f45918260645849b239ae",
      "b9f73d14822645c1ab51135eec082f6a",
      "b78f54efa5d442d480388dca14839cf9",
      "fba4069fea3640a589219b0e02fd78d6",
      "3daea6a7d287492a962f9b001ec148a4",
      "8987f0a1e1244cb1bf4bebefaa152ef5",
      "0a69b4ab61964ad2907e724af25141ac",
      "678a18e8470c468483653a2e40135b5c",
      "dbceb6433948459d812deb4e2fdbe481",
      "6c56204cb2f24bbf8202dc1cb60cb4a9",
      "7bc6ac375fa6431ba7065dbaa3ef7b28",
      "13fe2f1cd1cc4c31ba732b8a12ec1a55",
      "048df0a708f049548c535adddeb35eda",
      "3fa8777448724dac816f6b0f0ac86d14",
      "55669b0d1b2a44758282893f6149d5f1",
      "26908d703dee47fd8c6a53a479c2eb6f",
      "35011d82be8c4c7c925d9bfce72539b0",
      "0d79fa55ec6148e3b5a8e513d706ebcd",
      "267150d817b7487bb7b78f1aad02f7b7",
      "8abd4626b4074d0d8c17c724161bb726",
      "41bd82bedb514a29b056a6aaa31ec84a",
      "d680fd51bbb542b989750345e17485fb",
      "8ad6063244044da4860e6e63fb683e6e",
      "4befc482016f4dca8804d58ed8bdd404",
      "ee429ad77118416d816fd43b272b3dbf",
      "9104d147231f48b397fc5bbd794d48d7",
      "49ea7ea6ba624bde8fac862544fa76d8",
      "a762195aee304360bb42c434c4c39bf2",
      "6e9a62a45c2c433fb4e37c82ec26ba16",
      "0be57868fb4f46548391a9a431873d10",
      "0f01531168eb43a78168a383eb7fe7d8",
      "c088d90d52474ca9a3b7cbe3810746d3",
      "02d731d9256d44549b9a8f4550eb7568",
      "13b29499573f4178aaf9202a7b1d4b10",
      "62b9bcf0123e42ff80a27428f1249ca6",
      "7ad4e88f09694b5f9637dff6232940b9",
      "4083e60873b64e88bed67c81a6b7c2dc",
      "3670fe5a12d74b62ab4cd586b08b3605",
      "5cdab43d16f94ad0baca42aeb5253cf3",
      "76dc5b2e89024d06902297bfe1ff7adb",
      "f64299312a674993b1d35af645601c84",
      "2c69b661b7d74c96a4ac171325d94ef0",
      "0502d79e25544242ab20770262734954",
      "be1dbd5fcedf49d78dee3e2d56720d7d",
      "665cc0ab4cc94be49d38dbc644239dcc",
      "b2de8af05995400da6555cce713764ca",
      "540bff93d2584457b4e609527636872e",
      "732bd326d21841a392acdcdffaac0458",
      "2b1e891649f047ab95be0f81b086662f",
      "c4532e32ea874265a3a60fdab23c64d8",
      "fe77268b0c2946e4a95e1b9fcb5b05d4",
      "67fe1fa1f023405b8a88eaf99c032507",
      "b4af586309d34c83b598fb044e031a23",
      "07edf38631cf4f40bbcbc830dd28ae48",
      "3e64411bf84e41ccb32ff2b64a201df8",
      "4dfc55821d7e49d989f6257d3ab0d42c",
      "cc19b275be5e4f7aa7e8db732311b0c8",
      "b56d337a696446f3b78781c530c52feb",
      "b72cff0ab386442fa39e7a3c5f08888b",
      "8a5ca7475dea42b09842a03c6b49f095",
      "4d19991417d04644b905831a3996d940",
      "855e9bfb89a0469486d4683e9c834a08",
      "f1ea5ee7651243fda25dd2b411306b64",
      "70aadd8a8a35405ea41e1a68b8ad78e1",
      "4632c9534c66490c970adf0a8482a35b",
      "f10d6709773d4436b6c6eb74de777051",
      "da22e0c986ae449ca4af58b1fcf266ff",
      "50a74364561b40d5985ae49e040b91ff",
      "f4690a5cd6374ca2bf119244f08967e5",
      "63076dbba55441459f2f640f09e59f3c",
      "70697a9935df4390912de5f1cb849d0a",
      "9944c7efd16244be8a427d6c808f629c",
      "a28872e4bd414a4b98d93f1646a8519d",
      "b586e38a6eee4066960c9fa181415ca2",
      "80052dd5ba1e487c80cdfc26fb7c79bd",
      "70f84e088fcc4c049d629bfd53993139",
      "6da195cac8d24f899eab023a8ce86591",
      "7bafb577954c4bbea35e211f4abc6402",
      "8657785785a242dcb65973449a83b523",
      "580e0b041fd34c74aec5a496e0d87b4a",
      "a22cfe2af19b439abd16e3d4ac7d6d47",
      "bb6fd5350cb549969b9375278f86cdba",
      "8eccbcbea80f4b73b9bd21d5641816e0",
      "e7c7391155044dee86a1d5ff98447698",
      "907b33fa473d45f28c19f955875f3d92",
      "03849fb15d974cab9a234b23040eb6e7",
      "5bdc803df7964c9cad780a51b9e598d5",
      "18c942212d3344148e01b309cc04c96b",
      "471367d6b79a448293d3b8b07f8d47a7",
      "a7893ee063cd4df8ac429470899b5a1c",
      "b249857329b04fb6afc03d9a9eaa45aa",
      "e11b052d833a493d91ffb990d75db329",
      "ed2f620de3be4f41bdb15387ec0becde",
      "f1632690991d4da9b393e82439c89f0d",
      "c5dd8ac88c7843fbac0f46e129428647",
      "6166574f2c4343deac78f71b20444863"
     ]
    },
    "id": "B7pJgEwAYeZi",
    "outputId": "2e94ecd4-be7b-4848-ecf0-0168e00395e9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if USE_BF16_INSTEAD_OF_4BIT:\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=getattr(__import__('torch'),'bfloat16'))\n",
    "else:\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', load_in_4bit=True)\n",
    "del m\n",
    "print('prefetch ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccMs8kV3YeZi"
   },
   "source": [
    "### per‑user assets: lexicon + FAISS RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3c4d26ed08644d8991944832c8134e46",
      "35c2ed6546b74164b277601b6a96f79f",
      "24bc8563f4c64ef699de2c373f51716b",
      "5a83a51b2d2e44efbce96bff16084140",
      "b75f6b95ead54bf1b680bb6e09a20a61",
      "24d4aa6a718349719357261859bce63f",
      "e0f8f0c531d44ea888c8e6bf56d4000b",
      "9958171a6cf24119b5456260ec73c71f",
      "fcd41cddde604690828ee190df484d5f",
      "f176a6362a354cb58b2f6ff5e7e12c54",
      "f73e165c433941ce9c90c283851853d4",
      "0a8288c9d81d4c729eeb0d88793bf388",
      "b751b667af0b4796963524165c9d4510",
      "94edfceee86d4460a4b70a1310f05c82",
      "9bd3d3dd3e53402eb6531c0cc5f69c27",
      "c837c01c2776419bbaeb954dd3afde08",
      "7471ba3d74c34024b3d507890c0daae8",
      "e2fab5b1d028495e80d7dc8543f1ff2b",
      "38b044dc493d4bfa814bbd5962a9434b",
      "9bfe6445f8194ef6a13f7f6dc775eb35",
      "cc11302694474f198520f176fe63a53b",
      "5a856db526ed4415b7ac3f1dbc2add2a",
      "cf04224985af4c8abf0ab9ca72d93189",
      "6059a67b9c4141888a9296eef5909306",
      "ab6b5b1e5c764c86a8b7f3c9f898e706",
      "66e71a9bec714f94af042d157999c55b",
      "3fbe662741d542be829206e5f4049f58",
      "6663118d5f4b4391816488df89c81d4e",
      "29a2b26c26d64e1d97bc518ee0f64959",
      "6b367575d6af4a42a7c3ed0f12b36aa1",
      "9fb5718c6f30448eb0f06e6d0622ff9c",
      "32dbf7f4e43f49c1a37fe1de8f6506ac",
      "b75be2b3faa74a88bd02ee0b3e1dc78d",
      "e90dc648e286491a9c1c91ce7793282f",
      "f424962471e8426cb5a93bf833154acd",
      "fbe8390b74584452a4203b0cf3d418a4",
      "76c2af4a302a43e2baa23330ed42412d",
      "ead023f0490d4d90bc38233584de2cc8",
      "d32cdd34e02a47489d3b4e1b47a28bc7",
      "70414c603ace471daec21c346207130b",
      "445b61c18d1a4c709c5a26af8fe116b6",
      "9a07569d5d70439bb38617b26364afc8",
      "db973c1ddec5409d9012a3a1c6c05b9e",
      "bc3ca487953841b9a69126eb1fe4d12d",
      "e4d41b4995ab4e89882b15353ee0ec95",
      "05add42d2f96483fb9d93143f7a3c27e",
      "7870097c76204d79bb1867528e75292e",
      "29bb98d4f76547c5820959e5bc991abb",
      "64a97293de784e0d84e6ac1d1a946bdd",
      "dc89c3ccf6a344e1a76fcdb4286e7527",
      "6a376fd3993541658364c1a7babb2fca",
      "a69405d39e054ffd99c51aea54eb99d1",
      "52c7f162c0804dd28aac11e273fdbf0f",
      "9c747fdbb9d64429a735aa63a9119258",
      "3f6d11ff8d7b4761899b1f83182affe6",
      "fd723e2fc83143aea5be51ec94a7d66c",
      "ee48399f201341c0a9eb016d3af0d8ae",
      "914c9e9439b24329914e7139c8f17de6",
      "74d3631d230742cd84618eb4a04fa5ed",
      "27bf7c4d481441d89ed9e27b80c8a35e",
      "df64ccaa9af84f06942253c96f22bd47",
      "5870592830674b92a5576fba845c5247",
      "180bc18569584fe4ae7d72d0e3650bd4",
      "7021882a8b34428eb9409ce74daae5a7",
      "0a9576777b0f44329548cc879270b2b3",
      "6616e8bb406f4148ba9e1f7d67803dae",
      "8354295ae50c46b5b20e8f19bda17a30",
      "7d7b4b36eb9c46e383769ad0a93c75dc",
      "ae781d2fb1914d5cbff7a90c3c0a6bd2",
      "2c8bc14c4e9d4448a69851e67fe98c84",
      "3b066e3c96f74f2b8e9e1ec3dae154b2",
      "1f0375948e2a4ec3baff277ae7d8fdae",
      "8cb4bf7f81b44397b884deb172dba3c3",
      "32bd7417ecca409cabd38d45a6c4e51b",
      "0924acf584fd4868839efccbca0c2336",
      "ed898e5bf25042f58da0e79dc8aceb67",
      "50de1198ce6b45ee8a1d8870df597581",
      "0eb62bef220d4fd28fa927b3cbb7d485",
      "a0e2b18133d04d93ab069901a8467cd7",
      "606c41a9ac404b7b8302ae3c6cd9938e",
      "785360a23c0a4a12ab86154100b3ae7c",
      "3e38c40735ec47558d9d375bb8310e23",
      "2d7862c4797a4342aa1b2e8b37dd0107",
      "d714568d09d040d5b89b8bda87792e75",
      "afbfd8d5c30a4d4f9dc2bd338910ad5a",
      "52eb9eaf4b4c44929c6ce69bfbf14c17",
      "affe9949e11146efb2f4e8deebe10173",
      "47214181d9684dc8a00f04e0b787a058",
      "58332bf577fa4f3e8dda8c472c92aaf2",
      "15fd2e19bbb945418c16572e3ea9e89c",
      "cc77a6bafe6e4829a66d7d4e3e383602",
      "6545f8652c1a454688f99f4636f91acb",
      "007c9cb4d1914a279f876e2b970a52b0",
      "8f0243a7d4234e0b918fe55af9aa22d0",
      "40fb44f5a3d14b01bf1ebbb3ddf77bad",
      "4bfc64a97d8d453b82d54c786f5108c3",
      "2406e7da47df4db493f94795e16e9360",
      "99dc7be4f016464a8a126e737f71d025",
      "1fae12fee8ac4f2da8dd05197de7ba4c",
      "0eab6385c772494984a0716d23d90e34",
      "145ee97aac254f7a8d0c8d4bbd75170e",
      "a1602a33ffaf46069c85f0b2d6806315",
      "d4d2f2d7906b428c805bce936a59fd35",
      "948dfd883e8e40a0a1f23a82e1f393c7",
      "af7d3ce2c6434edf8e40954dceeb5af9",
      "e81ab82b7e484f58ba3d1e1098f2eb89",
      "9cf757dd44b74c92a3811981e18ec185",
      "bfbb0a9044444d7392bfb7f6f4809be6",
      "19edd7d079814b59b6f2cbc925ed0f92",
      "6e383233b7734aa9bfab897f5c79bca0",
      "f5aaa538ddbc432fa1d3ee33ebf0b608",
      "e4338820f7404f4889af0eb1fa28ad5c",
      "50bece30db6649ba8b0180612991916d",
      "d3aea9176e6c42fdbe18091437a8d4a3",
      "eee50cbeac8f495d8fda93bc15aa1eca",
      "f09947716a1a470daecf24d2c52b94f5",
      "e3da00bcd4e64bbcb955889a5aec356c",
      "480b0c9e0911460db5e38c21452cb5fc",
      "0474bfbea5db4b8891a14714444a91ae",
      "67183a9bb29543eca3a6980979e11a6c",
      "bcbe75fe7977476db267ef073c8c260f"
     ]
    },
    "id": "HiI9e78EYeZi",
    "outputId": "fe4793a0-0444-4b8a-f427-2e9be2852f0a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "sys.argv = ['lex','--users_dir', str(root/'users'), '--out_dir', str(root/'lexicons'), '--max_items','4000']\n",
    "from src.lexicon.build_lexicon import main as lex; lex()\n",
    "sys.argv = ['rag','--users_dir', str(root/'users'), '--out_dir', str(root/'rag'), '--model_name','sentence-transformers/all-MiniLM-L6-v2', '--chunk_chars','300','--overlap','50']\n",
    "from src.rag.build_rag import main as rag; rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f-jH9NLYeZi"
   },
   "source": [
    "### LoRA per user (resumable ckpts; bf16 toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "86770f5c44fa4eb1a45f3b58e2d0090c",
      "e19cd1d2275f4e5c9b41b3c9e4af0591",
      "c7be020f83694900aa2a8f64e47220e5",
      "b7f793c09d0c4db0b5b22cc2d67acf2c",
      "3785fbdb67e94930b29c1d6130c7e666",
      "31139593326e46c88d7aa4287acc2ab7",
      "6e87d56f54834fc3b85594d916888402",
      "877cb6bb73cc49969c007d526e73e94d",
      "559bb15cb8854abba867427c0007e42b",
      "3ded78e542124032a685d4594dc04cec",
      "bcfcdf35f36548198460d9180a716155"
     ]
    },
    "id": "1Sl6jlYsYeZi",
    "outputId": "af28b953-c3ad-43d4-c570-eeeb31204a9e"
   },
   "outputs": [],
   "source": [
    "# restore HF Auto classes, hard-reload our trainer, run training\n",
    "\n",
    "import os, sys, importlib\n",
    "import transformers  # currently mutated\n",
    "\n",
    "# 1) full refresh of transformers so AutoModelForCausalLM has its real .from_pretrained again\n",
    "transformers = importlib.reload(transformers)\n",
    "import transformers.models.auto.modeling_auto as modeling_auto\n",
    "importlib.reload(modeling_auto)\n",
    "import transformers.modeling_utils as modeling_utils\n",
    "importlib.reload(modeling_utils)\n",
    "from transformers import AutoModelForCausalLM  # fresh class now\n",
    "\n",
    "# 2) set dtype mode for train_lora.py (4-bit NF4 by default; flip to bf16 if you set the flag earlier)\n",
    "use_bf16 = globals().get('USE_BF16_INSTEAD_OF_4BIT', False)\n",
    "os.environ[\"TRAIN_DTYPE\"] = \"bf16\" if use_bf16 else \"4bit\"\n",
    "print(\"TRAIN_DTYPE:\", os.environ[\"TRAIN_DTYPE\"])\n",
    "\n",
    "# 3) purge old trainer import so it re-imports transformers AFTER our reload\n",
    "for k in list(sys.modules):\n",
    "    if k.startswith('src.lora.train_lora'):\n",
    "        del sys.modules[k]\n",
    "\n",
    "# 4) run training with your existing knobs\n",
    "from pathlib import Path\n",
    "root = Path('/content/drive/MyDrive/assistive_keyboard_7B')\n",
    "from src.lora.train_lora import main as train\n",
    "\n",
    "sys.argv = [\n",
    "  'train',\n",
    "  '--users_dir',    str(root/'users'),\n",
    "  '--adapters_dir', str(root/'adapters'),\n",
    "  '--base_model',   globals().get('BASE_MODEL', 'Qwen/Qwen2.5-7B-Instruct'),\n",
    "  '--rank',         str(globals().get('LORA_RANK', 8)),\n",
    "  '--alpha',        str(globals().get('LORA_ALPHA', 16)),\n",
    "  '--dropout',      str(globals().get('LORA_DROPOUT', 0.05)),\n",
    "  '--lr',           '2e-4',\n",
    "  '--steps',        str(globals().get('LORA_STEPS', 600)),\n",
    "  '--block_size',   '256'\n",
    "]\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlN0fYIc8iJP",
    "outputId": "c651b21f-5d72-416b-d8d8-f77297f7c02c"
   },
   "outputs": [],
   "source": [
    "# build users_active = finished adapters + top-20 unfinished (by adapt tokens)\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ROOT = Path(\"/content/drive/MyDrive/assistive_keyboard_7B\")\n",
    "BASE = globals().get(\"BASE_MODEL\",\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "finished=[]\n",
    "AD=ROOT/\"adapters\"\n",
    "if AD.exists():\n",
    "    for p in AD.iterdir():\n",
    "        if not p.is_dir(): continue\n",
    "        if (p/\"lora_adapter\").exists() or (p/\"checkpoint-600\").exists():\n",
    "            finished.append(p.name)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE, use_fast=True)\n",
    "cands=[]\n",
    "for d in sorted(p for p in (ROOT/\"users\").iterdir() if p.is_dir()):\n",
    "    if d.name in finished: continue\n",
    "    n = len(tok((d/\"adapt.txt\").read_text(encoding=\"utf-8\"), add_special_tokens=False)[\"input_ids\"])\n",
    "    cands.append((n, d.name))\n",
    "cands.sort(reverse=True)\n",
    "\n",
    "N=20\n",
    "sel = finished + [name for _,name in cands[:max(0, N-len(finished))]]\n",
    "print(f\"cohort size: {len(sel)}  (finished={len(finished)})\")\n",
    "\n",
    "UA = ROOT/\"users_active\"; UA.mkdir(exist_ok=True)\n",
    "for name in sel:\n",
    "    src = ROOT/\"users\"/name\n",
    "    dst = UA/name\n",
    "    if dst.exists(): continue\n",
    "    try: os.symlink(src, dst)\n",
    "    except Exception: shutil.copytree(src, dst)\n",
    "print(\"users_active ready:\", len(list(UA.iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e76bc6a0abb648258aa6ebf11bc21828",
      "0e817595da2c4911b68d8833234ee8db",
      "3ca58c2f27654e54bcf7dabc4150fa3b",
      "7e5ab0a0cfb446db9f4b3f63109ec168",
      "80a03da5969e4a93a4b2d93139bdc9a4",
      "d03d86232528438d9f40e43bee55c21e",
      "6cba678802cb4f2c8ed1fc68f4f53509",
      "2b3d4efba67e4e938d986cd60f9a8e5f",
      "4eb55428244449ecad7d1b4cbaee9b86",
      "3accf1e323624a788cce1fcd671041fc",
      "c43f68649bb64fb4bdd1fb2e545b8ae1"
     ]
    },
    "id": "5MEJTLo8-xLy",
    "outputId": "8bd9cbb4-d6ec-4bd6-91d1-9404b1c7b169"
   },
   "outputs": [],
   "source": [
    "# Clean overwrite of train_lora.py (PEFT LoRA, bf16, single-GPU, skip-finished, safe resume)\n",
    "from pathlib import Path\n",
    "import textwrap, sys, importlib, os, shutil, re\n",
    "\n",
    "ROOT = Path(\"/content/drive/MyDrive/assistive_keyboard_7B\")\n",
    "TRAINER = ROOT/\"code\"/\"src/lora/train_lora.py\"\n",
    "\n",
    "TRAINER.write_text(textwrap.dedent(\"\"\"\n",
    "import argparse, os, re, shutil\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "def read_txt(p):\n",
    "    return Path(p).read_text(encoding='utf-8')\n",
    "\n",
    "def mk_blocks(ids, bs=256):\n",
    "    if not ids:\n",
    "        return [[0]*bs]\n",
    "    return [ids[i:i+bs] for i in range(0, max(len(ids)-bs, 1), bs)] or [ids[:bs]]\n",
    "\n",
    "def mk_ds(txt, tok, bs=256):\n",
    "    ids = tok(txt, add_special_tokens=False)[\"input_ids\"]\n",
    "    return Dataset.from_dict({\"input_ids\": mk_blocks(ids, bs)})\n",
    "\n",
    "def step_from_ckpt_dir(p: Path):\n",
    "    m = re.search(r\"checkpoint-(\\\\d+)$\", p.name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def latest_ckpt(d: Path):\n",
    "    if not d.exists(): return None\n",
    "    cks = [p for p in d.glob(\"checkpoint-*\") if p.is_dir()]\n",
    "    return max(cks, key=step_from_ckpt_dir) if cks else None\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--users_dir', required=True)\n",
    "    ap.add_argument('--adapters_dir', required=True)\n",
    "    ap.add_argument('--base_model', default='Qwen/Qwen2.5-7B-Instruct')\n",
    "    ap.add_argument('--rank', type=int, default=8)\n",
    "    ap.add_argument('--alpha', type=int, default=16)\n",
    "    ap.add_argument('--dropout', type=float, default=0.05)\n",
    "    ap.add_argument('--lr', type=float, default=2e-4)\n",
    "    ap.add_argument('--steps', type=int, default=600)\n",
    "    ap.add_argument('--block_size', type=int, default=256)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # base model on single A100 in bf16\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model, device_map={'':0}, dtype=torch.bfloat16\n",
    "    )\n",
    "    base.config.use_cache = False  # don't warn during training\n",
    "\n",
    "    users = sorted(p for p in Path(args.users_dir).iterdir() if p.is_dir())\n",
    "    for u in users:\n",
    "        a = u/'adapt.txt'; v = u/'val.txt'\n",
    "        if not a.exists() or not v.exists():\n",
    "            continue\n",
    "\n",
    "        out_drive = Path(args.adapters_dir)/u.name\n",
    "        out_drive.mkdir(parents=True, exist_ok=True)\n",
    "        out_tmp = Path('/content/adapters_tmp')/u.name\n",
    "        out_tmp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # skip authors already finished\n",
    "        if (out_drive/'lora_adapter').exists() or (out_drive/'checkpoint-600').exists():\n",
    "            print('skip finished:', u.name)\n",
    "            continue\n",
    "\n",
    "        # wrap LoRA freshly per author\n",
    "        model = get_peft_model(base, LoraConfig(\n",
    "            r=args.rank, lora_alpha=args.alpha, lora_dropout=args.dropout,\n",
    "            bias='none', task_type='CAUSAL_LM'\n",
    "        ))\n",
    "\n",
    "        tr = mk_ds(read_txt(a), tok, args.block_size)\n",
    "        dv = mk_ds(read_txt(v), tok, args.block_size)\n",
    "\n",
    "        targs = TrainingArguments(\n",
    "            output_dir=str(out_tmp),\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=args.lr,\n",
    "            max_steps=args.steps,\n",
    "            eval_strategy='steps',\n",
    "            eval_steps=args.steps,    # eval once at the end\n",
    "            save_strategy='steps',\n",
    "            save_steps=args.steps,    # save once at the end\n",
    "            save_total_limit=1,\n",
    "            logging_steps=50,\n",
    "            report_to=[],\n",
    "            bf16=True,\n",
    "            dataloader_num_workers=2,\n",
    "            dataloader_pin_memory=True,\n",
    "        )\n",
    "\n",
    "        def collate(batch):\n",
    "            feats = [b['input_ids'] for b in (batch if isinstance(batch, list) else [batch])]\n",
    "            ml = max(len(f) for f in feats)\n",
    "            pad = tok.pad_token_id or 0\n",
    "            ids = [f + [pad]*(ml-len(f)) for f in feats]\n",
    "            t = torch.tensor(ids, dtype=torch.long)\n",
    "            attn = (t != pad).long()\n",
    "            return {'input_ids': t, 'attention_mask': attn, 'labels': t}\n",
    "\n",
    "        # resume: prefer tmp ckpt; fallback to old drive ckpt; skip if already at >= steps\n",
    "        ck = latest_ckpt(out_tmp) or latest_ckpt(out_drive)\n",
    "        if ck and step_from_ckpt_dir(ck) >= args.steps and (out_drive/'lora_adapter').exists():\n",
    "            print('skip finished:', u.name)\n",
    "            continue\n",
    "\n",
    "        trainer = Trainer(model=model, args=targs, train_dataset=tr, eval_dataset=dv, data_collator=collate)\n",
    "        trainer.train(resume_from_checkpoint=str(ck) if ck and step_from_ckpt_dir(ck) < args.steps else None)\n",
    "\n",
    "        # save adapter to /content then copy to Drive\n",
    "        model.save_pretrained(str(out_tmp/'lora_adapter'))\n",
    "        try:\n",
    "            shutil.rmtree(str(out_drive/'lora_adapter'), ignore_errors=True)\n",
    "            shutil.copytree(str(out_tmp/'lora_adapter'), str(out_drive/'lora_adapter'))\n",
    "        except Exception as e:\n",
    "            print('copy adapter failed:', e)\n",
    "\n",
    "        print('adapter:', out_drive/'lora_adapter')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"), encoding=\"utf-8\")\n",
    "\n",
    "# fresh import & run\n",
    "for k in list(sys.modules):\n",
    "    if k.startswith('src.lora.train_lora'):\n",
    "        del sys.modules[k]\n",
    "importlib.invalidate_caches()\n",
    "from src.lora.train_lora import main as train\n",
    "\n",
    "users_dir = ROOT/('users_active' if (ROOT/'users_active').exists() else 'users')\n",
    "sys.argv = [\n",
    "  'train',\n",
    "  '--users_dir',    str(users_dir),\n",
    "  '--adapters_dir', str(ROOT/'adapters'),\n",
    "  '--base_model',   globals().get('BASE_MODEL','Qwen/Qwen2.5-7B-Instruct'),\n",
    "  '--rank',         str(globals().get('LORA_RANK',8)),\n",
    "  '--alpha',        str(globals().get('LORA_ALPHA',16)),\n",
    "  '--dropout',      str(globals().get('LORA_DROPOUT',0.05)),\n",
    "  '--lr',           '2e-4',\n",
    "  '--steps',        str(globals().get('LORA_STEPS',600)),\n",
    "  '--block_size',   '256'\n",
    "]\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB3zrQmb9vhE",
    "outputId": "5cea2a2c-fc61-4e73-86ac-1bc444020c09"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "echo \"== scan for project roots ==\"\n",
    "find /content/drive -type d -name assistive_keyboard_7B 2>/dev/null | sed 's/^/DRIVE: /' || true\n",
    "find /content      -maxdepth 2 -type d -name assistive_keyboard_7B 2>/dev/null | sed 's/^/LOCAL: /' || true\n",
    "\n",
    "echo -e \"\\n== scan for lora_adapter dirs (first 20) ==\"\n",
    "find /content/drive -type d -name lora_adapter 2>/dev/null | head -n 20 | sed 's/^/DRIVE: /' || true\n",
    "find /content      -type d -name lora_adapter 2>/dev/null | head -n 20 | sed 's/^/LOCAL: /' || true\n",
    "\n",
    "echo -e \"\\n== scan for users splits (folders with adapt.txt) (first 20) ==\"\n",
    "find /content/drive -type f -name adapt.txt 2>/dev/null | head -n 20 | sed 's/^/DRIVE: /' || true\n",
    "find /content      -type f -name adapt.txt 2>/dev/null | head -n 20 | sed 's/^/LOCAL: /' || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVyFUfKEYeZi"
   },
   "source": [
    "### eval (ngram, llm_base, llm_lex, llm_full) → CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fceda55e0cc54bd3a50738ef304b831d",
      "43cd865a3abb4369a1539a6ded812d9e",
      "65146e65b6f14977915734931d5cb49b",
      "38d942846b9d4de1ab64e4863eddb96e",
      "26f1e5cac7634ef9bde96f38ff1cf9b9",
      "dac0dab746ce4e75b02e8ce42b3e3af3",
      "2cb9f8971b1e4ee59ce1b2ca019b30cf",
      "d43a7853015444bbb5e4ce6c73759dea",
      "eecb3d2a82144b16b74156701f02f1c0",
      "343f513277d542e18e7f0dd2907f9e32",
      "67e938580fe74b8f9eac52542964420c",
      "cd96d4fd67244966ba0ee12404e14717",
      "89285252d76d479fa976273acfc360d7",
      "410de7476d5d4c7796e9d0be6437799d",
      "55ffcc10a7fc47c090e9df67ce58905c",
      "9ef1dca759b7482e9fb4d0143e57ff1c",
      "da87141f12be4691b48a4d67301f4437",
      "e2383ae041d0483484f613c70dc51034",
      "7d4e890c6fbc41878c2b8d73c98bb9f3",
      "0d99c99da3654a409ab6fe181005b1b9",
      "dde73350964b4058aa57e7617a1404f6",
      "670887c3f1ec46329cc4332bd2720e92",
      "2c073eb672c646e3ab5f1e0cb749df7e",
      "4278b8b031254c969b0151df2d9a608c",
      "5ea8439c5e5d4161b784d5ac946cff15",
      "4adee1b40265436391c3e068d272c934",
      "35ffda845ca14e5d8723d53382ae014c",
      "ab3b0f3abfdb4cd5a5445f246d7c16e8",
      "202f4d175f40415a950e66ad9ea42813",
      "288199a9922a4ccf8bce1b1a52531a86",
      "40e1ec4efccb499a81590e9df3487cca",
      "51fa1bc281bf41f487f81b519c47c283",
      "b5bcfc6f9b0f46eb92de736844408827",
      "bfb4d59bdd754ce7baa5ee2e5c3b6a64",
      "2c30cc9aec1243589fee52e5825613e1",
      "fd1ffa04a5814d1db2ec96bb24c30d1d",
      "d2869fd1e32b4e58b3c6315a228cd413",
      "5ec87ce4f4434415bebbaa80a81f41e5",
      "2733d4a102d2432da5b02dd02ee0e7e7",
      "9627159fce7042b8b97e0188dbf2747f",
      "ea0db5baf10c4a998e6dc7fb306c1180",
      "45f106dcbbf0450298765f468bf7f6f1",
      "d57967ad2483451591dcc09d1a444495",
      "d034f7aa75364d2a85f6e6f2c995d3fc",
      "3aee02d99cdd4dc4ab2055a8d5d986be",
      "04e6d060140d4d25aa6629750445584a",
      "5a75220b28684e109f89948826f1cea2",
      "852540049fc2473589e1b62bc3c857f8",
      "ce321b6f14184c7580db4c34fdca7d24",
      "ad682d2dd9354c2b87cd380d6944bf68",
      "c5224e81076141b1a97efdb60f149414",
      "b80df5aa77a147cd85c666ce98a9942c",
      "a6e8cecc21db46e882d8365ac9feba0d",
      "2699586454ba46e29c0aac0f3806a7dc",
      "fec90014b28440cfa3199f3556966220",
      "2ebf529b09e64f149962bd7294b091d0",
      "e347aaa82bf24717bff2aa6dd6c71d05",
      "25b2ec41f3194edd88cb0190338dc2b2",
      "28c52d054c5343b799f2ff3179fe31cf",
      "17495d8620094fdd951247489cc3fbea",
      "012559c91a734da2ad789ca33c9ed544",
      "f35e2d6b2d00495caca64f16395485de",
      "eb993f6fd93c464da2b1d3d6c44e23b0",
      "68186f2fc9b443229629389dc66c0096",
      "66c935aac3c54e9f96044961df86af2f",
      "4021cee23b1c4b7fb1e6c5d8e1fd3055",
      "ca2c395efeee45bf9e7cabce0a3462f3",
      "59862ec3fd824e7a9daead30de826d72",
      "552e0ffc253e4f639a8b46811f4f0f0f",
      "abc2d6fb18914714b2cc4fa9f218f035",
      "f8d3b5c378c94698865fa899f2d18832",
      "aa605d15f8b843168592059cfcfa16ff",
      "a4442892948f4a38b4dc7fb86f2fc96e",
      "48dcff1b07c044cba3135886a532f4b7",
      "4db0a15a505745cfa1ec88bcf9a55a0c",
      "d6702b2ce0ed45538ae3e1b422a2bbbf",
      "5f1d290ada434b2eb04701fbd424c679",
      "52d34c1b8e1b44808eb1e311e0cd0e7f",
      "ff223088f79c4975ad3b610163a4507c",
      "33722c22f41b4b708fea11a9ac11a1b5",
      "e432945585cc4ca989b66e151b23e710",
      "9503ad685b8c4dadaaacd55c683e7252",
      "31902862a63449e3bbf563f4f185e829",
      "666aa0901cc24339ab6d75d08954925d",
      "a97830ef36314af6be3f994752f5e50b",
      "a61fccd28cf54e66928bd3d0e205c55f",
      "5903c317c1b948ac9077176012497f0d",
      "a52272c92cba49f7a25f4aba258b9730",
      "d3bba03df85b4316b43e93ec111c26f9",
      "f22edd81c89b4d36aae9b5ab95d1fcf2",
      "3737c55ef8d6469ebe4c0ff5f2e96b04",
      "adb89861512648d5bb3a5fc0250ddbe8",
      "277970d83195405983ef274e02b0046c",
      "4cb2426b9e4d41eea37a8d8d8ed7784a",
      "4f9931ca618144829576b64c8d7dd8a7",
      "1eda535bd8dd4f16a18af7aff4a38bc1",
      "eeeaefe31d7e4dfca4d0e3c33bb7c327",
      "b950cf21cb3d453fa108fff732d034f0",
      "16e565fbb1064ef2bc5883d5a3bd46ba",
      "551aa8b3e90642df969092392e65426c",
      "4280e6d307944557ac6f6d484218b8b9",
      "21f88431e509414b9a373a5b73c20bf6",
      "d89a03597f364e9c990436b2994b4fbb",
      "3a64561709a64e2684989139c5aac023",
      "d2ce771fd11f40e4868ab969e935b4dd",
      "85b95b4633bb441b9097c8092e94a641",
      "919db8142e6d45bdbcf79c6beccf8466",
      "513e7f07494047bda76c0a4f368496a5",
      "88c54d3fd4a14d4aa869986ed096d437",
      "af7e314b75634c39bdf080e9ec01a8e3",
      "36196a7ad07b463bb074acfe13f49126",
      "1a41f16e10be44b2aa3551632e65dfa4",
      "00243fdf17874b979db798015319517e",
      "faf07bb718ad4987a2a60127772e0a34",
      "522ca318fe8c4a04a9d828624e1a268b",
      "992b9fce5af944e5ae5743a7f7af37d5",
      "c7c18608871147c882c4b2bbe9dda827",
      "20fb4224e872449db572b2f3e78b6d99",
      "ba3a35663c1a45e2b4bd70f96665b5fe",
      "276b23759a2f4c80a0ec48a433c70f20",
      "fc4271dd3ff245ff8cad008ef2271d63",
      "086377dcb0624dc58806a616b35be5d9",
      "f5701763bfdf4c569c4d8af0797f2f68",
      "3d35f019ec97463284360a3efa717c10",
      "b7e2745b32df4d479255763713957f4f",
      "ae10569c95f341c8bdf588779250c291",
      "a03f06242b334b4d88ea847b656d0b6d",
      "b4a0db72f266470c9cd5c8f8fce2d942",
      "255c70a55195462f90b884e876a7b348",
      "4c0328e465ea4716af8353db01002730",
      "ebc613da9be04381b53ed423bf1a4e9c",
      "a856eeb640164e0784703de47065fb9d",
      "e4809d14bdec4b85b62a9ff21cb57ebf",
      "87040370a7544638a4e41d57fe5f5eea",
      "3e0da61fdd9c41a8a9fce7ae5157240a",
      "c874f3d6c0724c1fabb0733028a70006",
      "330e3aad3741479495effde0d7b8de08",
      "4b9d2c71420e4ce1ae4a40c5fad0ace6",
      "73a83d338db8428696cbbc5d1d909d93",
      "67176e34397c426483a76d5d99f35103",
      "61b5e1a947534789aa6f0ea7869006ca",
      "70c5f4b8559e40d3bd75ce0766c01fd5",
      "86679f665c4047ee8eee9f8b85808c11",
      "cb81031bebc846729c20efa411652532",
      "5fa1993e56ac40bd86d434135a5a7d6e",
      "9cd4bf811b2746dcb746f55bbaff2a13",
      "2b82fb4087034fc1a944f1568d959ec5",
      "ce34971e91504657b7c1bb90124f6a00",
      "6bbfe857d9d04c95b4b23326ecdb66b7",
      "70b5a38d2ef54697bcb1f31cfbde8bc6",
      "d1dae25fe3584c0b894eb0d35d2c1952",
      "004fa6641c3946ea9087b7fff25a093d",
      "c36ed9f37aeb4cb28fe9ea513463910f",
      "e37045d6798b49f9968fe948222914c4",
      "891224aafa94456c9eb40c0d1efdfaf2",
      "65e74cd44d474a53ad33e9566fb87f37",
      "0da8e520724f4488b0a0186f017e75e1",
      "491daf2e239b4602a672c45d51c4bd52",
      "3b5e6923d0bc4cc7b82c455a1e792310",
      "7897079935a64c9fafaf27df9ef26aed",
      "d1aa4c88829c42569d46935f221cdb21",
      "1c99232923e54c77ba0175c84bea4131",
      "fd8af5d643bd425f9f124ea4bc49015e",
      "32bf154077b94882b1748ef1529b8a25",
      "364548b894514228a86673f9261555b5",
      "9bd6624101694245a7e79eb1d8847a9a",
      "bb2be017b1fa4fd58313841774bc5496",
      "e9a1c7601ff54fa68a4978228fd23782",
      "f85ed5dc38884fe9bffa6a66e925b318",
      "979d308e71884ef3ba2300e5ad0f030e",
      "27d451f2149844308b50d10b486539fe",
      "34b6eae410794a4aa1e17a2612cfcbe0",
      "00dc124be13b4bbfbacde60b79d95f2a",
      "1490344f8f0b43a8a792177670b56007",
      "d99b80958a444c95baff6cde44593668",
      "28e20e240fcb4918b0da88be8a58e3e7",
      "11fd8fc132004b00a0c44853a30b78a2",
      "7a6cfc054a4e4efabcb309b50cc148da",
      "60268974855f49b48a579f95a9d9e843",
      "3ed875e13236499fa7ef614fd6a957f4",
      "9c4e13b4bd614a079957c8eed09e1465",
      "434ba7cbfb1847aba50f6c0e02897694",
      "5e805b4ff5a54801bc46136b33a7b125",
      "80ba2fc285bc44ed9f98fdf979000d3c",
      "05ee9a2a74b2427fb75932ef2ee4cb3f",
      "03c60eb4e17b4d8081af9bcda12fc61a",
      "294cd631da654429818e154fcad60146",
      "7de72f3cdb8949c29127e3fd83d67217",
      "6a4b16140fb941aaa9f4089b6dec443e",
      "40b127d46f7647d4a2a23ab948e9e001",
      "cf1b477d019948aabe5ca7a74239f575",
      "927b7ed0d7b14dafa7839bcdf6284b72",
      "be61db13976141d9aee8088d4e398427",
      "d1fa2854ce6a4a62b153026f7f85ff3a",
      "358a7fec653244ac8f962b79cf3bde3f",
      "ffa7710237714868a83c0ef61879d132",
      "afd889dfef624c579d54c1799afc85f4",
      "234bbc593797466fa8123bddae8c65bc",
      "7afd95ff33d44b1987c81a68ed44210a",
      "95e2e67d6db34bcabebe5c4f66f3e20f",
      "48c7899393244281af32047a8842a1ae",
      "72f2072ced4e4d8caa468adeadd7ca22",
      "2b3c8d82095e4c5884e8ed6936ae720d",
      "4f7297c26936478e8986398bf3bb779a",
      "21be06c570e747b8a4aaa010c32d440c",
      "af81642862854f20b91cfb471b758dee",
      "412384b0b47c49ccb026c392641d5360",
      "63f36bccca434995883e34e205b4a5ec",
      "a9890f4ffab54f84b9752b9454d66e43",
      "000f47dcfa864e1ea72b5e41b8f744d3",
      "57adc3095dc94f45bc9153494e606933",
      "502b5a2836eb477d93dce2a5c0e41e96",
      "0c5be830be8647c08113eb4c471c254d",
      "13f540d28c45449ab1d55088d2dae2e8",
      "ecdc6d9967e04931937292eee9962d2a",
      "5c75c3868ddb400ca78687567e846a0b",
      "73fd1c0786d2433d8d646a92a73100ad",
      "3fd5f63745234d7b90efbb535b3296bb",
      "5e11507f77514c568c3c40b5240f9240",
      "a08ff2c8e3924b9c9e9d926ac4558f2d",
      "7662f2c125b24a05b2db15b9a57f49b5",
      "f4b046e1c3db495da47cab0fe84db1a8",
      "0f2d47ea43874ac4a54189c58a728212",
      "7e3dab092b6043a1adf7d1f1deab36dc",
      "626b6c19ada4479281fcfa08898d60a2",
      "d256484ed1b64218911b833f3be63329",
      "358a52574fab4e32a1acc078191098d6",
      "e3f0aa6f769b43b1ae35c6cc9c23e56a",
      "59cbe2233fd44e9f88208d337af4d946",
      "05b90d3c47974345ae66e3debd55eb98",
      "f6434c9bc6ca4651aff178b9e293ec59",
      "758ab27b65cf4934a8b6acd48d26352b",
      "b29b17df2f1e49888b7ac484b557cd17",
      "b8ebce1259064226acaafb2b7764a47f",
      "bd16064e70174a3daaae48ef3e4554b9",
      "163665b33ac9449a9c6b78cc83dfa41a",
      "22c5535c6fa64862b5275177047ad522",
      "54c5ad2042d24d43b41dc4bfb6c03ded",
      "c8d59f2fe8404d02ab7074beb90803cf",
      "7132c3b1df8c48468f984dfa34a438ca",
      "8aa07385376744ddb062c2ef1e310dce",
      "08c12cf2a5a14864a7ddcee5f3841d06",
      "8a06b60d796746e68baea4a340c8e0fe",
      "433a356bdc774598bca798065e2163e2",
      "163fd076ae714af489c9f5dbcdc76da8",
      "6cce12abb56c4d7da12392603e271d87",
      "2726851ff4f340f69b40a63e349df927",
      "3411413256894314a4ce38c85a2a81c8",
      "200c964d84d240a2b4be932e79bc2101",
      "c56bbef016294dfca54a46aa29a63609",
      "257758ff917f47f18d7e8220bb16416d",
      "7f5df72281904a31acbe89e765b43a31",
      "ca39e5ce82864dedbc4b19ce64b6a3bc",
      "4167fe1aedc64630aa09a9247edf5503",
      "ad88c44d0652464d89ca1af2c830f3bf",
      "fed24c64bb194273842e316ea8b0ec2e",
      "4ec349a66ba541f497aab7b9db3dabae",
      "384abc65d32b441b9e1d09b4e99b7183",
      "12e152f572bb4fad892cf67aace9a244",
      "db9a1cfddd114a9990397d6eb184f372",
      "af884cffd50e4c56a241be0f501a1c77",
      "eec7a9365aed4b6e8cf48c80ea54adc8",
      "810874c2ad5e4cb19f73d3f1261b8135",
      "c6b21276df7241f4987618764b76e385",
      "051bae456677482b9c68a927e12bf07d",
      "72c9dbf6ae9b477c8f57731da2c5b89a",
      "9dd1bed89ec34ad3adba66dd156618fe",
      "5103802922c14fa19c0f95de29d85409",
      "cf1991c73e504b50a69b701a4d94a24d",
      "727547a4a4ab49e783f343bcf888db3f",
      "0b2fd909dabf402283c39cd98da876e8",
      "e7048b8e1b8e4f888d56564144ce0f31",
      "d33e8560391b4b15ba08a947f4be2462",
      "98812cfdfb9946f98217b3c9f9d6853f",
      "a093c0cc0fc244118f802c1c04040d47",
      "6bafcb5bb8ab4f01bc41eebcd05494b0",
      "b855865c49074b1a972bd2714c11df0e",
      "565d0717db4a40498050d24c4e6746f0",
      "719942d03f9a4e2d8ecec2b3b38dd139",
      "460a6e9fc22b4b588682b3df38fc13fc",
      "a2b07e20cd144d7daa4e2a8bee5c8bf2",
      "e6548fc69afa455d984a213a3bab3410",
      "287db112d6814193877cdecce14501bf",
      "1335cc11e4494caaae5b2d0e81e6d720",
      "be9fe361645a41ce938b826d10598d10",
      "dee8ae469d0d47a98d1d0b79fb9e61f3",
      "f5c271b2460540aa92aba630544a7f0f",
      "08d547d93320400bab007770c86dfc9b",
      "b98ab08bf0c34842a1386aa935db3666",
      "97f4557ca3cb4572bbd96fb27c52adc7",
      "9a70d20652ae477f9a25bc37292a964d",
      "a5133c4f6c41469d8a14b44a86362ff2",
      "3788d5ec7b284d3c9281e4dbbc2c8b8d",
      "cd016e04fce14b3f91101cd34837c5b5",
      "3a5287587f3b4dc1b44f2e131bc94edb",
      "21a019ab255c48b080ac4b25962e8cdb",
      "0f2601a830844382ac83a75e521ccfa2",
      "1c97909ba3884ab0970f627fc0c43763",
      "bc38e7b0ea5647ac9b15a2fb57dd6e4a",
      "a5b3ea4b5a2d4fb888a3e7ef3dfa13ac",
      "c784fa9b14d44c34beadc6d71892341d",
      "ea1fe9643aa247b8b13bcc91e4d2410c",
      "a924c8a1b50e4adc9e2eff69235cc184",
      "46e5ae452f7f4893afaf7ec9ef0afaf9",
      "2d928d69182c4a54b731ceed49ec5ee8",
      "bac1c592b5f74d5b91400694fca129b3",
      "c2d4afa65f194f328067f5add460d2ea",
      "a9a719d96cd241ca8b444d4e0224b313",
      "234c7b54f2764bf6949bfaaed373110b",
      "590e328795cd4fdd9027fa09ce0557b9",
      "e0fe27127fda4bc6b238e8438172143a",
      "08def90785af42caa8e0826d2ef7b042",
      "b31f339e01174b498e63df0f3dcd71e0",
      "026dcdc52a5a436faf1fa72b18556f43",
      "43b06ce010fe48219c88a88814da37f0",
      "f142bff580e143fdb80433fce3d32a4c",
      "80a022e0a82d4f2fbcdb728d10bb46cf",
      "c9ed1e33e2c04c5db424005f2a4489b6",
      "af9111d85e7c467981bc9a9fda553c1d",
      "51522acd6ec8403cac657c7992786889",
      "aaa8d036acc34230885321bb00b3dce8",
      "35c1913497cf4d7ab6fd9d39f64a2ca5",
      "86e734e1ea544283ae2de0ef26741e83",
      "2c5675ce44bb4a14b812c45b57e73ea7",
      "d462a4a5a4f842b3888725f115b71aaf",
      "8c1de4ef059449c794e3d44391c374fd",
      "fd949480e57e452b94a525f0c5283cca",
      "69704832badd4a4a814bdfb46dcb0d87",
      "e783f34524554f00a4899c449d583b1f",
      "77888fb8dfce4baca210d3c1cdd81783",
      "bdd5c92f9f05470881ee6f62c517eb3a",
      "2b72db29d1e54e9586c0b63735036057",
      "b2b1c8b1ae0e4cfd9df5bfdfe4d70b47",
      "3cf5e12694124c13af7e892a4b5568c2",
      "25f8c842759b44a3a29b8b1397a9ad9d",
      "8f6a8712c88345f18c13790d1bad4399",
      "e1176f1530bd42a2806bb60afc9e6669",
      "087d2cbe84e94bf28e041409e982045e",
      "be9cdded7c1f4158b54100375e2ebd2c",
      "cd38ee21122543c88fba794c3d0cb827",
      "70c08807b1394cf594e40e69f688ed2c",
      "e94ba3eda64e45b0836c0cec9abe4d65",
      "176b62a3b9b64037b0b4f5d6da9b2bdd",
      "e3b4441702ae448eb778347f21132c33",
      "9b25490f230546ac90d7fb839fda4afe",
      "fda442afea3a4e22b6f988e16960559d",
      "6f128eb1184c4c2abe3cc30c78fc18a3",
      "a2f650f7b93a4260b75a0558f3bd1894",
      "3c01f82ae0c64af6a64db8e9c066ccd7",
      "755eeeba7f7344258e84288c530510d1",
      "54f48978c0834914bd5918fbb582f4d5",
      "36eb611bcf3c4657bda09284ce647c33",
      "bc7396cd479b45529d8b0ed87d4c68f0",
      "21043a1575124cba9171fd5371425480",
      "2504b923c8c2452f8045d28c089b8044",
      "8e50f70ed0c5434c9fe6d89a8f131d0d",
      "70ada0202d954b5db108d3665cbc28ec",
      "f074a528a76646ab91f89c31410e4d4e",
      "a814688961534136861423814d8603eb",
      "c9b6bc52d55f4a2daf156d658aff3a87",
      "231c3f930a5b4653a43c09339f3bb2ad",
      "7f1e36a3fe9c46f094b3eddb3dda3d8f",
      "e4c1bd5760214b029db09a2963e5691b"
     ]
    },
    "id": "t_m9MMNpYeZi",
    "outputId": "74942c35-408f-4df6-e51e-2edc920a2261"
   },
   "outputs": [],
   "source": [
    "# eval adapters-only cohort → writes CSVs under runs/ and prints summary\n",
    "from pathlib import Path\n",
    "import os, json, time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# libs\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Missing deps (transformers/peft). In this Colab, pip install them once.\")\n",
    "\n",
    "ROOT = Path(\"/content/drive/MyDrive/assistive_keyboard_7B\")\n",
    "USERS, ADAP, UA, LEX, RUNS = ROOT/'users', ROOT/'adapters', ROOT/'users_active', ROOT/'lexicons', ROOT/'runs'\n",
    "RUNS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE      = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
    "MAX_TOK    = 4096  # cap eval length so we don't blow up on giant test.txt\n",
    "\n",
    "# build users_active from authors that have an adapter\n",
    "names = [p.name for p in ADAP.iterdir()\n",
    "         if (p/'lora_adapter'/'adapter_model.safetensors').exists() and (USERS/p.name).exists()]\n",
    "import shutil\n",
    "shutil.rmtree(UA, ignore_errors=True); UA.mkdir(parents=True, exist_ok=True)\n",
    "for a in names:\n",
    "    src, dst = USERS/a, UA/a\n",
    "    try: os.symlink(src, dst)\n",
    "    except Exception: shutil.copytree(src, dst)\n",
    "print(f\"users_active (adapters-only): {len(names)} authors\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "def ids_text(text, max_tok=MAX_TOK):\n",
    "    return tok(text, add_special_tokens=False, truncation=True, max_length=max_tok)[\"input_ids\"]\n",
    "\n",
    "def ids_file(fp: Path, max_tok=MAX_TOK):\n",
    "    txt = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return ids_text(txt, max_tok=max_tok)\n",
    "\n",
    "def lex_prefix(author):\n",
    "    f = LEX/f\"{author}.lexicon.json\"\n",
    "    if not f.exists(): return []\n",
    "    try: obj = json.loads(f.read_text(encoding=\"utf-8\") or \"{}\")\n",
    "    except: obj = {}\n",
    "    keys = list(obj.keys())[:10]\n",
    "    return tok(\"Style hints: \" + \", \".join(keys) + \"\\n\\n\", add_special_tokens=False)[\"input_ids\"] if keys else []\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_llm(text_ids, model, prefix=None):\n",
    "    prefix = prefix or []\n",
    "    X = torch.tensor([ (prefix + text_ids)[:MAX_TOK] ], dtype=torch.long, device=DEVICE)\n",
    "    logits = model(input_ids=X).logits\n",
    "    pred = logits[:, :-1].argmax(dim=-1); gold = X[:, 1:]\n",
    "    return (pred == gold).float().mean().item()\n",
    "\n",
    "def eval_ngram(adapt_ids, test_ids):\n",
    "    from collections import defaultdict, Counter\n",
    "    nxt = defaultdict(Counter)\n",
    "    for a,b in zip(adapt_ids[:-1], adapt_ids[1:]): nxt[a][b]+=1\n",
    "    fb = Counter(adapt_ids[1:]).most_common(1)[0][0] if len(adapt_ids)>1 else 0\n",
    "    corr=tot=0; prev=None\n",
    "    for t in test_ids[:MAX_TOK]:\n",
    "        if prev is None: prev=t; continue\n",
    "        pred = (nxt[prev].most_common(1)[0][0] if nxt[prev] else fb)\n",
    "        corr += int(pred==t); tot+=1; prev=t\n",
    "    return corr/max(1,tot)\n",
    "\n",
    "def load_base():\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={'':0} if DEVICE=='cuda' else None, dtype=DTYPE)\n",
    "    m.eval(); return m\n",
    "def load_with_adapter(author):\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={'':0} if DEVICE=='cuda' else None, dtype=DTYPE)\n",
    "    m = PeftModel.from_pretrained(m, str(ADAP/author/'lora_adapter'))\n",
    "    m.eval(); return m\n",
    "\n",
    "BASE = load_base()\n",
    "rows = {k:[] for k in ['ngram','llm_base','llm_lex','llm_full']}\n",
    "authors = sorted([p.name for p in UA.iterdir() if p.is_dir()])\n",
    "\n",
    "t0_all = time.time()\n",
    "for a in authors:\n",
    "    d = UA/a\n",
    "    adapt_ids = ids_file(d/'adapt.txt', max_tok=MAX_TOK)\n",
    "    test_ids  = ids_file(d/'test.txt',  max_tok=MAX_TOK)\n",
    "\n",
    "    # ngram baseline\n",
    "    t0=time.time(); acc = eval_ngram(adapt_ids, test_ids)\n",
    "    rows['ngram'].append({'user':a,'kss':acc*100,'accepts':acc,'time_ms':(time.time()-t0)*1000})\n",
    "\n",
    "    # base model\n",
    "    t0=time.time(); acc = eval_llm(test_ids, BASE, None)\n",
    "    rows['llm_base'].append({'user':a,'kss':acc*100,'accepts':acc,'time_ms':(time.time()-t0)*1000})\n",
    "\n",
    "    # base + lex\n",
    "    pref = lex_prefix(a)\n",
    "    t0=time.time(); acc = eval_llm(test_ids, BASE, pref)\n",
    "    rows['llm_lex'].append({'user':a,'kss':acc*100,'accepts':acc,'time_ms':(time.time()-t0)*1000})\n",
    "\n",
    "    # LoRA adapter\n",
    "    M = load_with_adapter(a)\n",
    "    t0=time.time(); acc = eval_llm(test_ids, M, pref)\n",
    "    rows['llm_full'].append({'user':a,'kss':acc*100,'accepts':acc,'time_ms':(time.time()-t0)*1000})\n",
    "    del M; torch.cuda.empty_cache()\n",
    "\n",
    "# write CSVs\n",
    "for k,v in rows.items():\n",
    "    pd.DataFrame(v).to_csv(RUNS/f'leaderboard_{k}.csv', index=False)\n",
    "\n",
    "print(f\"done → {RUNS}   authors: {len(authors)}   time={int(time.time()-t0_all)}s\")\n",
    "\n",
    "# summary\n",
    "tbl = pd.read_csv(RUNS/'leaderboard_ngram.csv')[[\"user\",\"kss\",\"accepts\",\"time_ms\"]].rename(\n",
    "    columns={\"kss\":\"kss_ngram\",\"accepts\":\"acc_ngram\",\"time_ms\":\"time_ngram\"})\n",
    "for tag in [\"llm_base\",\"llm_lex\",\"llm_full\"]:\n",
    "    d = pd.read_csv(RUNS/f'leaderboard_{tag}.csv')[[\"user\",\"kss\",\"accepts\",\"time_ms\"]].rename(\n",
    "        columns={\"kss\":f\"kss_{tag}\",\"accepts\":f\"acc_{tag}\",\"time_ms\":f\"time_{tag}\"})\n",
    "    tbl = tbl.merge(d, on=\"user\", how=\"inner\")\n",
    "\n",
    "print(\"\\n== per-model means ==\")\n",
    "print(tbl.drop(columns=[\"user\"]).mean(numeric_only=True).round(3))\n",
    "print(\"\\n== winner counts ==\")\n",
    "print(tbl.set_index(\"user\")[['kss_ngram','kss_llm_base','kss_llm_lex','kss_llm_full']].idxmax(axis=1).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaW4YVK6YeZi"
   },
   "source": [
    "### results (means + per‑author spread + outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVe3tncYYeZi",
    "outputId": "2946d009-cb89-41bf-be8b-d9006dda56a4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "root='/content/drive/MyDrive/assistive_keyboard_7B/runs'\n",
    "dfs=[]\n",
    "for f in ['leaderboard_ngram.csv','leaderboard_llm_base.csv','leaderboard_llm_lex.csv','leaderboard_llm_full.csv']:\n",
    "    p=os.path.join(root,f)\n",
    "    try: dfs.append(pd.read_csv(p).assign(model=f.replace('leaderboard_','').replace('.csv','')))\n",
    "    except Exception as e: print('missing', p, e)\n",
    "res=pd.concat(dfs, ignore_index=True)\n",
    "print('== means by model ==')\n",
    "print(res.groupby('model')[['kss','time_ms','accepts']].mean().round(3))\n",
    "full=res[res['model']=='llm_full'][['user','kss']]\n",
    "print('\\n== per-author KSS (llm_full) describe ==')\n",
    "print(full.describe().round(3))\n",
    "med=full['kss'].median(); mad=(full['kss']-med).abs().median()\n",
    "bad=full[full['kss']<med-1.5*mad]['user'].tolist()\n",
    "print('\\noutliers (low KSS):', bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn_9fiwZYeZi"
   },
   "source": [
    "### quick live check (single author suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "miVmhDw5YeZi",
    "outputId": "35ed6ccf-e3a8-4988-d89d-03bbd6d33cd5"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "from src.infer.suggest import LLM, Lex, RAG\n",
    "from transformers import AutoTokenizer\n",
    "authors=[p.name for p in (root/'users').iterdir() if p.is_dir()]\n",
    "author=authors[0] if authors else None\n",
    "print('author:', author)\n",
    "lex=None; lpath=root/'lexicons'/f'{author}.lexicon.json'\n",
    "if lpath.exists():\n",
    "    lex=Lex(AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True), lpath.read_text(encoding='utf-8'), cap=2.5)\n",
    "rag=None; f=root/'rag'/f'{author}.faiss'; c=root/'rag'/f'{author}.chunks.json'\n",
    "if f.exists() and c.exists():\n",
    "    rag=RAG(f, c)\n",
    "adapter=str(root/'adapters'/author/'lora_adapter')\n",
    "sg=LLM(BASE_MODEL, adapter_dir=adapter, rag=rag, lex=lex, bf16=USE_BF16_INSTEAD_OF_4BIT)\n",
    "ctx='Hi team, following up on the budget approval for Q4. If we can align by Friday,'\n",
    "print('ctx:', ctx)\n",
    "print('suggestions:', sg.suggest(ctx, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xVB7dRElHAt_",
    "outputId": "807388f3-b5cc-4bb3-98e8-6f6be0a85987"
   },
   "outputs": [],
   "source": [
    "# === setup ===\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path(\"/content/drive/MyDrive/assistive_keyboard_7B\")\n",
    "USERS = ROOT/'users'\n",
    "RUNS  = ROOT/'runs'\n",
    "PLOT  = RUNS/'plots'\n",
    "PLOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === load leaderboards ===\n",
    "paths = {\n",
    "    'ngram': RUNS/'leaderboard_ngram.csv',\n",
    "    'llm_base': RUNS/'leaderboard_llm_base.csv',\n",
    "    'llm_lex': RUNS/'leaderboard_llm_lex.csv',\n",
    "    'llm_full': RUNS/'leaderboard_llm_full.csv',\n",
    "}\n",
    "dfs = []\n",
    "for name, p in paths.items():\n",
    "    d = pd.read_csv(p)\n",
    "    d['model'] = name\n",
    "    dfs.append(d)\n",
    "res = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ==== Plot 1: Per-model mean KSS ====\n",
    "means = res.groupby('model')['kss'].mean().reindex(['ngram','llm_base','llm_lex','llm_full'])\n",
    "plt.figure(figsize=(6,4))\n",
    "means.plot(kind='bar')\n",
    "plt.ylabel('KSS (top-1 next-token %)')\n",
    "plt.title('Mean KSS by Model (20 authors)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'kss_means_by_model.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ==== Plot 2: Per-author KSS distribution (box) ====\n",
    "plt.figure(figsize=(7,4))\n",
    "res.boxplot(column='kss', by='model', grid=False)\n",
    "plt.suptitle('')\n",
    "plt.title('Per-Author KSS Distribution')\n",
    "plt.ylabel('KSS (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'kss_box_by_model.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ==== Plot 3: Winner counts ====\n",
    "w = (res.pivot_table(index='user', columns='model', values='kss')\n",
    "       .idxmax(axis=1).value_counts()\n",
    "       .reindex(['ngram','llm_base','llm_lex','llm_full']).fillna(0))\n",
    "plt.figure(figsize=(6,4))\n",
    "w.plot(kind='bar')\n",
    "plt.ylabel('# Authors won')\n",
    "plt.title('Model Winner Counts (KSS best per author)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'winner_counts.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ==== Plot 4: Dumbbell (base vs LoRA) per author ====\n",
    "pivot = res.pivot_table(index='user', columns='model', values='kss')\n",
    "dd = pivot[['llm_base','llm_full']].dropna().sort_values('llm_base')\n",
    "plt.figure(figsize=(7,10))\n",
    "y = np.arange(len(dd))\n",
    "plt.hlines(y, dd['llm_full'], dd['llm_base'], lw=1)\n",
    "plt.plot(dd['llm_full'], y, 'o', label='LoRA', markersize=3)\n",
    "plt.plot(dd['llm_base'], y, 'o', label='Base', markersize=3)\n",
    "plt.yticks(y, dd.index)\n",
    "plt.xlabel('KSS (%)')\n",
    "plt.title('Per-Author: Base vs LoRA')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'dumbbell_base_vs_lora.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ==== Plot 5: Email length histograms (sample 9 authors) ====\n",
    "import random\n",
    "authors = sorted([p.name for p in USERS.iterdir() if p.is_dir()])\n",
    "sample = authors[:9] if len(authors)>=9 else authors\n",
    "fig, axes = plt.subplots(3,3, figsize=(10,8))\n",
    "axes = axes.ravel()\n",
    "for ax, a in zip(axes, sample):\n",
    "    txt = (USERS/a/'adapt.txt').read_text(encoding='utf-8', errors='ignore')\n",
    "    lens = [len(x) for x in txt.splitlines() if x.strip()]\n",
    "    if len(lens) > 5000:\n",
    "        lens = lens[:5000]  # cap for speed\n",
    "    ax.hist(lens, bins=40)\n",
    "    ax.set_title(a)\n",
    "    ax.set_xlabel('line length (chars)')\n",
    "    ax.set_ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'length_hist_sample9.png', dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved plots to:\", PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_SAGDIGHGGX",
    "outputId": "7a6f9a35-a58d-4d56-c5b5-bb6acc829007"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def top_bigrams(text, k=20):\n",
    "    import re\n",
    "    toks = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    bigrams = zip(toks, toks[1:])\n",
    "    return Counter([\" \".join(b) for b in bigrams]).most_common(k)\n",
    "\n",
    "a = res['user'].unique()[0]  # pick first author\n",
    "txt = (USERS/a/'adapt.txt').read_text(encoding='utf-8', errors='ignore')\n",
    "pairs = top_bigrams(txt, k=20)\n",
    "labels, values = zip(*pairs)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.barh(labels[::-1], values[::-1])\n",
    "plt.title(f\"Top bigrams — {a} (adapt)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/f'top_bigrams_{a}.png', dpi=200)\n",
    "plt.close()\n",
    "print(\"Saved:\", PLOT/f'top_bigrams_{a}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "d8e58c194a724873ba91b660496d8767",
      "41697d49199349c2b6c04ec5f098e4ff",
      "030a4345aede4618ad0f932e6ceea88a",
      "904901772b34401aac838dea26a1fcb0",
      "6e7d1427e8fe464ab4146b8824986d39",
      "467f32fcb5f847bea60741352523ad11",
      "9b78718f71c34a06a560a8703a39c679",
      "22b71a41592043948262f980b9decbb0",
      "e5d8d9af8b0d4b578f94a03e330fc628",
      "f8a7df366ca641f28756c5c40c097924",
      "d5caa4c5c38d46a99528df9a883958e5",
      "437dff70bd3847d8b861e7b38d9d6c9f",
      "2168d54f18a04e2ab3ab52328b876aa1",
      "59df923bad714dd38c4e390882a74fa6",
      "005d6946756745418131a905cbc29f6f",
      "49aa2c51d2594cfabb654f7e1a745f7d",
      "8017b901791547b6b3bc7f23e3cc6f3b",
      "3814e409ea1b4c89ba1b0423a39a69e9",
      "de6f7717b6074ed28016d18a1e434e26",
      "c54844bd2c0e4dfaad6e65f72dc74895",
      "1ad514093cb649a7b2c3589a905b169c",
      "231488b5b4d349ffb63a3148c41800ed",
      "33af7677090f4dc98369781fe2a62abf",
      "559a7ddf9bd743f3827edef8c6964802",
      "10be5f461d864d8a9d01dc05a79899f7",
      "e0ca8de0b8934ed29fc151653b6d8694",
      "2baaafcc1e524bfb912ca9b1d9519679",
      "80c593c4290b44a4966b6916bed8c8b3",
      "dc699ab57da34b66acadb5a2cd42bf39",
      "a6f5a2dcaf4f46b4ba525ada759b1c50",
      "e31becf26f6c46fe9d6db1bbb71133d1",
      "8d66a1fad46840a49eeb2a6c2fa8c9d0",
      "637c9b5a2aac4e61adab424f8d4b3a3d",
      "475c6ad43aee4799af630118bea86f2e",
      "83a79d0a82cd43b58beeed7e23001c47",
      "4911adcd2e184df89b29f85189716b25",
      "197a4523e37c4adea0c2a57fed2e1f2b",
      "061a2dbda96e4458992f83341ed17e34",
      "479d20a4207242728e06be84aba06b1d",
      "9eeb102a6b974f5a88e4f2bb51cbfb01",
      "73b3223ee8e24d2d80ee3dbbf30f720a",
      "f5fbe03e4f994f0cb88d0ed186cd0f30",
      "c41ff00429744d7f86c907c9c1597982",
      "f4840f740db14e78b7acea00374e5e2f",
      "c97fc03d59a3428291f22c4efd077b3b",
      "359ef3bc22514c9b932928bfb80a0105",
      "ba0408c6e92946c19a0887317de4fdc3",
      "89738e02207547778b8dace16ba5dbed",
      "056e0d42a3b34e94a76d0fafde5f4e92",
      "75ae53c9a68f4266a32ce41baf96848c",
      "0bd8643d625e4f299ee7775779694eeb",
      "9b54a18887dd4ab8b3bf1a5d0e7f61de",
      "91c00dde80a14c978a0e9e19d529a472",
      "d97f62c4a47b41ba9a81975dd4535a72",
      "a6c658ac11db46058bd2fd9d5a78f297",
      "590619b330a34d0b9f0027157d2b08e9",
      "d775eff2eead40b88ac4524b0e6ef4af",
      "0bc9a90926c8422ba60fb87c23956ebb",
      "534db085a9e54691b0df101d69884328",
      "4db93c3608d24cef8b8c77fc077865ca",
      "dba54d8a5b674e76ad47bd03247824d7",
      "fd583e6f64484d8f92c022afaa317025",
      "973d2a67ce844511b6910b4fa0dea529",
      "c07d93c7571547909b67583acfff64e2",
      "83007c172f984764aee19c7bb30b342e",
      "5a3cd06a40834daa88e38e8d90c1606c"
     ]
    },
    "id": "BQ2I0oUKHH-N",
    "outputId": "035fa17b-daf7-4659-bc1e-4d057c2a8e49"
   },
   "outputs": [],
   "source": [
    "# Requires: pip install umap-learn sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=str(ROOT/'hf_cache'))\n",
    "rows = []\n",
    "for a in authors[:10]:  # first 10 authors for speed\n",
    "    lines = (USERS/a/'adapt.txt').read_text(encoding='utf-8', errors='ignore').splitlines()\n",
    "    lines = [l for l in lines if len(l.split())>=5][:200]  # 200 samples/author\n",
    "    rows.extend([(a, l) for l in lines])\n",
    "labels = [r[0] for r in rows]\n",
    "sents  = [r[1] for r in rows]\n",
    "emb = model.encode(sents, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "proj = umap.UMAP(n_components=2, random_state=42).fit_transform(emb)\n",
    "plt.figure(figsize=(8,6))\n",
    "for a in sorted(set(labels)):\n",
    "    m = [i for i,l in enumerate(labels) if l==a]\n",
    "    plt.scatter(proj[m,0], proj[m,1], s=6, label=a, alpha=0.6)\n",
    "plt.legend(markerscale=3, bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "plt.title('UMAP of sentence embeddings (adapt, 10 authors)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT/'umap_authors.png', dpi=200)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

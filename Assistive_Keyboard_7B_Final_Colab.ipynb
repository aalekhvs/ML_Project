{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistive Keyboard — 7B (Colab A100, Drive‑persistent)\n",
    "\n",
    "- base: `Qwen/Qwen2.5-7B-Instruct`\n",
    "- Drive-backed code/data/adapters/results\n",
    "- per-user LoRA + lexicon + RAG; eval KSS/latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu sanity\n",
    "import sys, torch, platform\n",
    "print('py:', sys.version.split()[0], '| cuda:', torch.cuda.is_available(), '| plat:', platform.platform())\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount drive + dirs\n",
    "from google.colab import drive; drive.mount('/content/drive')\n",
    "from pathlib import Path; import os\n",
    "PROJ = Path('/content/drive/MyDrive/assistive_keyboard_7B'); PROJ.mkdir(parents=True, exist_ok=True)\n",
    "CODE = PROJ/'code'; CODE.mkdir(exist_ok=True)\n",
    "DATA = PROJ/'data'; (DATA/'processed').mkdir(parents=True, exist_ok=True)\n",
    "SPLITS = PROJ/'splits'; SPLITS.mkdir(exist_ok=True)\n",
    "USERS = PROJ/'users'; USERS.mkdir(exist_ok=True)\n",
    "LEX = PROJ/'lexicons'; LEX.mkdir(exist_ok=True)\n",
    "RAGD = PROJ/'rag'; RAGD.mkdir(exist_ok=True)\n",
    "ADAPT = PROJ/'adapters'; ADAPT.mkdir(exist_ok=True)\n",
    "RUNS = PROJ/'runs'; RUNS.mkdir(exist_ok=True)\n",
    "CACHE = PROJ/'hf_cache'; CACHE.mkdir(exist_ok=True)\n",
    "os.environ['HF_HOME'] = str(CACHE)\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(CACHE)\n",
    "print('root:', PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config (demo knobs)\n",
    "MAX_TEST_AUTHORS = 20\n",
    "ADAPT_TOKENS     = 2000\n",
    "VAL_TOKENS       = 800\n",
    "TEST_TOKENS      = 2000\n",
    "LORA_STEPS       = 600\n",
    "LORA_RANK        = 8\n",
    "LORA_ALPHA       = 16\n",
    "LORA_DROPOUT     = 0.05\n",
    "BASE_MODEL       = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "USE_BF16_INSTEAD_OF_4BIT = False  # flip if 4bit acts up; A100-80G can do bf16 easily\n",
    "SEED = 42\n",
    "\n",
    "# seeds\n",
    "import os, random, numpy as np, torch\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deps\n",
    "%%bash\n",
    "set -e\n",
    "pip -q install --upgrade pip\n",
    "pip -q install numpy pandas tqdm pyyaml regex scikit-learn ujson\n",
    "pip -q install transformers accelerate datasets sentence-transformers\n",
    "pip -q install faiss-cpu peft bitsandbytes bert-score mauve-text\n",
    "python - <<'PY'\n",
    "import torch; print('torch', torch.__version__, 'cuda?', torch.cuda.is_available())\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project code → Drive\n",
    "from pathlib import Path; import textwrap\n",
    "for sub in ['src/utils','src/data','src/splits','src/lexicon','src/rag','src/lora','src/infer','src/eval']:\n",
    "    (CODE/sub).mkdir(parents=True, exist_ok=True)\n",
    "(CODE/'src/__init__.py').write_text('')\n",
    "(CODE/'src/utils/__init__.py').write_text('')\n",
    "(CODE/'src/utils/io.py').write_text(textwrap.dedent('''\n",
    "from pathlib import Path\n",
    "import json, ujson\n",
    "def read_lines(p):\n",
    "    return Path(p).read_text(encoding='utf-8').splitlines()\n",
    "def write_lines(p, lines):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(p).write_text('\\n'.join(lines), encoding='utf-8')\n",
    "def read_jsonl(p):\n",
    "    out=[]\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line: out.append(json.loads(line))\n",
    "    return out\n",
    "def write_jsonl(p, rows):\n",
    "    Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p,'w',encoding='utf-8') as f:\n",
    "        for r in rows: f.write(ujson.dumps(r, ensure_ascii=False)+'\\n')\n",
    "'''))\n",
    "(CODE/'src/data/__init__.py').write_text('')\n",
    "(CODE/'src/data/clean.py').write_text(textwrap.dedent(r'''\n",
    "import re\n",
    "QUOTE_RE = re.compile(r'(?m)^(>+).*?$')\n",
    "SIG_RE = re.compile(r'(?ims)--\\s*\\n.*?$')\n",
    "def clean_text(s:str)->str:\n",
    "    s=s.replace('\\r\\n','\\n')\n",
    "    s=re.sub(QUOTE_RE,'',s)\n",
    "    s=re.sub(SIG_RE,'',s)\n",
    "    s=re.sub(r'[ \\t]+',' ',s)\n",
    "    s=re.sub(r'\\n{3,}','\\n\\n',s)\n",
    "    return s.strip()\n",
    "def approx_token_count(s:str)->int:\n",
    "    return len(re.findall(r\"\\w+|[.,!?;:]\", s))\n",
    "'''))\n",
    "(CODE/'src/data/enron_loader.py').write_text(textwrap.dedent(r'''\n",
    "from pathlib import Path\n",
    "from .clean import clean_text, approx_token_count\n",
    "from src.utils.io import write_jsonl\n",
    "def build_authors_jsonl(maildir_root: str, out_jsonl: str, min_doc_tokens: int = 20):\n",
    "    rows=[]; maildir=Path(maildir_root)\n",
    "    for user_dir in maildir.iterdir():\n",
    "        if not user_dir.is_dir(): continue\n",
    "        author_id=user_dir.name\n",
    "        for p in user_dir.rglob('*'):\n",
    "            if not p.is_file(): continue\n",
    "            try: txt=p.read_text(errors='ignore')\n",
    "            except Exception: continue\n",
    "            txt=clean_text(txt)\n",
    "            if approx_token_count(txt)>=min_doc_tokens:\n",
    "                rows.append({'author_id':author_id,'doc_id':str(p.relative_to(maildir)),'text':txt})\n",
    "    write_jsonl(out_jsonl, rows); print(f'wrote {len(rows)} docs → {out_jsonl}')\n",
    "'''))\n",
    "(CODE/'src/splits/__init__.py').write_text('')\n",
    "(CODE/'src/splits/make_splits.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from src.utils.io import read_jsonl, write_lines\n",
    "from src.data.clean import approx_token_count\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--authors_jsonl', required=True)\n",
    "    ap.add_argument('--out_dir', required=True)\n",
    "    ap.add_argument('--min_tokens', type=int, default=4000)\n",
    "    ap.add_argument('--adapt_tokens', type=int, default=2000)\n",
    "    ap.add_argument('--val_tokens', type=int, default=800)\n",
    "    ap.add_argument('--test_tokens', type=int, default=2000)\n",
    "    ap.add_argument('--max_test_authors', type=int, default=4)\n",
    "    ap.add_argument('--seed', type=int, default=42)\n",
    "    args=ap.parse_args(); random.seed(args.seed)\n",
    "    out_dir=Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows=read_jsonl(args.authors_jsonl)\n",
    "    by_author=defaultdict(list)\n",
    "    for r in rows: by_author[r['author_id']].append(r['text'])\n",
    "    kept={}\n",
    "    for a,docs in by_author.items():\n",
    "        tot=sum(approx_token_count(t) for t in docs)\n",
    "        if tot>=args.min_tokens: kept[a]=docs\n",
    "    authors=sorted(kept.keys()); random.shuffle(authors)\n",
    "    n=len(authors); n_train=int(0.70*n); n_dev=int(0.15*n)\n",
    "    train_ids=authors[:n_train]; dev_ids=authors[n_train:n_train+n_dev]; test_ids=authors[n_train+n_dev:]\n",
    "    test_ids=test_ids[:args.max_test_authors]\n",
    "    write_lines(out_dir/'authors_train.txt', train_ids)\n",
    "    write_lines(out_dir/'authors_dev.txt', dev_ids)\n",
    "    write_lines(out_dir/'authors_test.txt', test_ids)\n",
    "    users_dir=Path(str(Path(out_dir).parent/'users')); users_dir.mkdir(exist_ok=True)\n",
    "    for a in test_ids:\n",
    "        texts=kept[a][:]; random.shuffle(texts)\n",
    "        acc=0; adapt=[]; val=[]; test=[]\n",
    "        for t in texts:\n",
    "            tc=approx_token_count(t)\n",
    "            if acc<args.adapt_tokens: adapt.append(t); acc+=tc\n",
    "            elif acc<args.adapt_tokens+args.val_tokens: val.append(t); acc+=tc\n",
    "            else: test.append(t)\n",
    "        udir=users_dir/a; udir.mkdir(parents=True, exist_ok=True)\n",
    "        (udir/'adapt.txt').write_text('\\n\\n'.join(adapt), encoding='utf-8')\n",
    "        (udir/'val.txt').write_text('\\n\\n'.join(val), encoding='utf-8')\n",
    "        (udir/'test.txt').write_text('\\n\\n'.join(test), encoding='utf-8')\n",
    "    print(f'train/dev/test: {len(train_ids)}/{len(dev_ids)}/{len(test_ids)} | users/* ready')\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/lexicon/__init__.py').write_text('')\n",
    "(CODE/'src/lexicon/build_lexicon.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, json, re, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tok(s):\n",
    "    return re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\sA-Za-z0-9]\", s)\n",
    "def build_lex(text, k=4000):\n",
    "    v=TfidfVectorizer(tokenizer=tok, lowercase=True, ngram_range=(1,2), min_df=2, max_df=0.9, use_idf=True, smooth_idf=True, norm=None)\n",
    "    X=v.fit_transform(text.splitlines()); vocab=v.get_feature_names_out()\n",
    "    scores=np.asarray(X.sum(axis=0)).ravel(); idx=scores.argsort()[::-1]\n",
    "    top=[(vocab[i], float(scores[i])) for i in idx[:k]]\n",
    "    return {'entries':[{'token':t,'score':s} for t,s in top]}\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser(); ap.add_argument('--users_dir', required=True); ap.add_argument('--out_dir', required=True); ap.add_argument('--max_items', type=int, default=4000); args=ap.parse_args()\n",
    "    out=Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        p=u/'adapt.txt'\n",
    "        if not p.exists(): continue\n",
    "        text=p.read_text(encoding='utf-8'); lex=build_lex(text, args.max_items)\n",
    "        (out/f'{u.name}.lexicon.json').write_text(json.dumps(lex, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "        print('lex:', u.name)\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/rag/__init__.py').write_text('')\n",
    "(CODE/'src/rag/build_rag.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "def chunks(s, m=300, ov=50):\n",
    "    s=s.strip(); out=[]; i=0\n",
    "    while i<len(s): out.append(s[i:i+m]); i+=max(1, m-ov)\n",
    "    return out\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser(); ap.add_argument('--users_dir', required=True); ap.add_argument('--out_dir', required=True)\n",
    "    ap.add_argument('--model_name', default='sentence-transformers/all-MiniLM-L6-v2'); ap.add_argument('--chunk_chars', type=int, default=300); ap.add_argument('--overlap', type=int, default=50)\n",
    "    args=ap.parse_args(); emb=SentenceTransformer(args.model_name)\n",
    "    out=Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        p=u/'adapt.txt'\n",
    "        if not p.exists(): continue\n",
    "        cs=chunks(p.read_text(encoding='utf-8'), args.chunk_chars, args.overlap)\n",
    "        if not cs: continue\n",
    "        X=emb.encode(cs, batch_size=64, convert_to_numpy=True, show_progress_bar=False).astype(np.float32)\n",
    "        faiss.normalize_L2(X); idx=faiss.IndexFlatIP(X.shape[1]); idx.add(X)\n",
    "        faiss.write_index(idx, str(out/f'{u.name}.faiss'))\n",
    "        (out/f'{u.name}.chunks.json').write_text(json.dumps(cs, ensure_ascii=False), encoding='utf-8')\n",
    "        print('rag:', u.name, len(cs))\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/lora/__init__.py').write_text('')\n",
    "(CODE/'src/lora/train_lora.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, re\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "def read_txt(p): return Path(p).read_text(encoding='utf-8')\n",
    "def mk_ds(txt: str, tok, bs=256):\n",
    "    ids=tok(txt, return_tensors=None, truncation=False)['input_ids']\n",
    "    blocks=[ids[i:i+bs] for i in range(0, len(ids)-bs, bs)] or [ids[:bs]]\n",
    "    return Dataset.from_dict({'input_ids': blocks})\n",
    "def last_ck(d: Path):\n",
    "    c=[p for p in d.glob('checkpoint-*') if p.is_dir()]\n",
    "    if not c: return None\n",
    "    def step(p):\n",
    "        import re; m=re.search(r'checkpoint-(\\d+)', p.name); return int(m.group(1)) if m else -1\n",
    "    return sorted(c, key=step)[-1]\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--users_dir', required=True)\n",
    "    ap.add_argument('--adapters_dir', required=True)\n",
    "    ap.add_argument('--base_model', default='Qwen/Qwen2.5-7B-Instruct')\n",
    "    ap.add_argument('--rank', type=int, default=8)\n",
    "    ap.add_argument('--alpha', type=int, default=16)\n",
    "    ap.add_argument('--dropout', type=float, default=0.05)\n",
    "    ap.add_argument('--lr', type=float, default=2e-4)\n",
    "    ap.add_argument('--steps', type=int, default=300)\n",
    "    ap.add_argument('--block_size', type=int, default=256)\n",
    "    args=ap.parse_args()\n",
    "    tok=AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
    "    model=AutoModelForCausalLM.from_pretrained(args.base_model, device_map='auto', load_in_4bit=True, torch_dtype=torch.float16)\n",
    "    model=prepare_model_for_kbit_training(model)\n",
    "    model=get_peft_model(model, LoraConfig(r=args.rank, lora_alpha=args.alpha, lora_dropout=args.dropout, bias='none', task_type='CAUSAL_LM'))\n",
    "    for u in Path(args.users_dir).iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        a=u/'adapt.txt'; v=u/'val.txt'\n",
    "        if not a.exists() or not v.exists(): continue\n",
    "        out=Path(args.adapters_dir)/u.name; out.mkdir(parents=True, exist_ok=True)\n",
    "        tr=mk_ds(read_txt(a), tok, args.block_size); dv=mk_ds(read_txt(v), tok, args.block_size)\n",
    "        targs=TrainingArguments(output_dir=str(out), per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=8,\n",
    "            logging_steps=10, learning_rate=args.lr, max_steps=args.steps, evaluation_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=3, report_to='none')\n",
    "        def collate(batch):\n",
    "            feats=[b['input_ids'] for b in (batch if isinstance(batch,list) else [batch])]\n",
    "            ml=max(len(f) for f in feats); pad=tok.pad_token_id\n",
    "            ids=[f + [pad]*(ml-len(f)) for f in feats]\n",
    "            return {'input_ids': torch.tensor(ids), 'labels': torch.tensor(ids)}\n",
    "        ck=last_ck(out)\n",
    "        Trainer(model=model, args=targs, train_dataset=tr, eval_dataset=dv, data_collator=collate).train(resume_from_checkpoint=str(ck) if ck else None)\n",
    "        model.save_pretrained(str(out/'lora_adapter'))\n",
    "        print('adapter:', out/'lora_adapter')\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "(CODE/'src/infer/__init__.py').write_text('')\n",
    "(CODE/'src/infer/suggest.py').write_text(textwrap.dedent(r'''\n",
    "from collections import defaultdict, Counter\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "import numpy as np, torch\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def tok_basic(s): return re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[0-9]+|[^\\sA-Za-z0-9]\", s)\n",
    "class NGram:\n",
    "    def __init__(self, text: str, n: int = 3):\n",
    "        toks=tok_basic(text.lower()); self.n=n; self.ng=defaultdict(Counter)\n",
    "        for i in range(len(toks)-n): self.ng[tuple(toks[i:i+n-1])][toks[i+n-1]]+=1\n",
    "    def suggest(self, ctx: str, k: int = 3):\n",
    "        toks=tok_basic(ctx.lower()); key=tuple(toks[-(self.n-1):]) if len(toks)>=self.n-1 else tuple(toks)\n",
    "        cand=self.ng.get(key,{}); return [w for w,_ in cand.most_common(k)]\n",
    "class Bias(LogitsProcessor):\n",
    "    def __init__(self, mp): self.mp=mp or {}\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        if self.mp: scores[:, list(self.mp.keys())]+=torch.tensor(list(self.mp.values()), device=scores.device)\n",
    "        return scores\n",
    "class Lex:\n",
    "    def __init__(self, tok, lex_json, cap=2.5):\n",
    "        self.tok=tok; self.mp={}\n",
    "        try: entries=json.loads(lex_json)['entries']\n",
    "        except Exception: entries=[]\n",
    "        for e in entries[:2000]:\n",
    "            ids=self.tok(e['token'], add_special_tokens=False)['input_ids']\n",
    "            if len(ids)==1: self.mp[ids[0]]=cap\n",
    "    def proc(self): return Bias(self.mp)\n",
    "class RAG:\n",
    "    def __init__(self, f, c, em='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.idx=faiss.read_index(str(f)); self.ch=json.loads(Path(c).read_text(encoding='utf-8'))\n",
    "        self.emb=SentenceTransformer(em)\n",
    "    def top(self, text, k=4):\n",
    "        q=self.emb.encode([text], convert_to_numpy=True).astype(np.float32); faiss.normalize_L2(q); D,I=self.idx.search(q,k)\n",
    "        return [self.ch[i] for i in I[0] if i>=0]\n",
    "class LLM:\n",
    "    def __init__(self, base: str, adapter_dir: str=None, rag=None, lex=None, max_ctx=512, bf16=False):\n",
    "        self.tok=AutoTokenizer.from_pretrained(base, use_fast=True)\n",
    "        if bf16:\n",
    "            self.m=AutoModelForCausalLM.from_pretrained(base, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "        else:\n",
    "            self.m=AutoModelForCausalLM.from_pretrained(base, device_map='auto', load_in_4bit=True)\n",
    "        if adapter_dir and Path(adapter_dir).exists(): self.m=PeftModel.from_pretrained(self.m, adapter_dir)\n",
    "        self.rag=rag; self.lex=lex; self.max_ctx=max_ctx; self.m.eval()\n",
    "    def _prompt(self, tail, mem):\n",
    "        if mem:\n",
    "            bullets='\\n'.join(f'- {c[:200]}' for c in mem)\n",
    "            mem=f\"Memory\\n{bullets}\\n\\n\"\n",
    "        return f\"Continue in the user's style.\\n{mem}Draft:\\n{tail}\\n\\nContinue:\"\n",
    "    def suggest(self, ctx: str, k: int = 3):\n",
    "        tail=ctx[-1000:]; mem=self.rag.top(tail,4) if self.rag else []\n",
    "        prompt=self._prompt(tail, mem)\n",
    "        ids=self.tok(prompt, return_tensors='pt', truncation=True, max_length=self.max_ctx).to(self.m.device)\n",
    "        procs=[self.lex.proc()] if self.lex else None\n",
    "        with torch.no_grad():\n",
    "            out=self.m.generate(**ids, max_new_tokens=6, do_sample=False, num_beams=max(1,k), num_return_sequences=k,\n",
    "                                 logits_processor=procs, pad_token_id=self.tok.eos_token_id)\n",
    "        texts=self.tok.batch_decode(out[:, ids['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        res=[]\n",
    "        for t in texts:\n",
    "            t=t.strip(); m=re.match(r\"^\\S{1,8}\", t); s=m.group(0) if m else t[:8]\n",
    "            if s and s not in res: res.append(s)\n",
    "        return res[:k]\n",
    "'''))\n",
    "(CODE/'src/eval/__init__.py').write_text('')\n",
    "(CODE/'src/eval/typing_sim.py').write_text(textwrap.dedent(r'''\n",
    "import argparse, time, csv\n",
    "from pathlib import Path\n",
    "from src.infer.suggest import NGram, LLM, Lex, RAG\n",
    "def load_text(p): return Path(p).read_text(encoding='utf-8')\n",
    "def sim(doc: str, sugg, k=3, max_chunk=8):\n",
    "    kp=0; kw=0; acc=0; t0=time.time(); i=0\n",
    "    while i<len(doc):\n",
    "        kp+=1; kw+=1; i+=1\n",
    "        pref=doc[:i]; sug=sugg.suggest(pref, k=k)\n",
    "        if not sug: continue\n",
    "        remain=doc[i:]; ok=False\n",
    "        for s in sug:\n",
    "            s=s[:max_chunk]\n",
    "            if remain.lower().startswith(s.lower()):\n",
    "                saved=max(len(s)-1,0); kw+=1; kw-=saved; i+=len(s); acc+=1; ok=True; break\n",
    "        if not ok: continue\n",
    "    ms=(time.time()-t0)*1000.0; kss=1.0-(kw/max(kp,1))\n",
    "    return dict(keys_plain=kp, keys_with=kw, kss=kss, accepts=acc, time_ms=ms)\n",
    "def main():\n",
    "    ap=argparse.ArgumentParser()\n",
    "    ap.add_argument('--users_dir', required=True)\n",
    "    ap.add_argument('--mode', choices=['ngram','llm_base','llm_lex','llm_full'], default='ngram')\n",
    "    ap.add_argument('--base_model', default='Qwen/Qwen2.5-7B-Instruct')\n",
    "    ap.add_argument('--adapters_dir', default='adapters')\n",
    "    ap.add_argument('--lexicons_dir', default='lexicons')\n",
    "    ap.add_argument('--rag_dir', default='rag')\n",
    "    ap.add_argument('--results_csv', required=True)\n",
    "    ap.add_argument('--k', type=int, default=3)\n",
    "    ap.add_argument('--bf16', action='store_true')\n",
    "    args=ap.parse_args(); out=[]\n",
    "    users=Path(args.users_dir)\n",
    "    for u in users.iterdir():\n",
    "        if not u.is_dir(): continue\n",
    "        a=u/'adapt.txt'; t=u/'test.txt'\n",
    "        if not a.exists() or not t.exists(): continue\n",
    "        if args.mode=='ngram':\n",
    "            sg=NGram(load_text(a))\n",
    "        else:\n",
    "            from transformers import AutoTokenizer\n",
    "            lex=None\n",
    "            if args.mode in ('llm_lex','llm_full'):\n",
    "                lp=Path(args.lexicons_dir)/f'{u.name}.lexicon.json'\n",
    "                if lp.exists(): lex=Lex(AutoTokenizer.from_pretrained(args.base_model, use_fast=True), lp.read_text(encoding='utf-8'), cap=2.5)\n",
    "            rag=None\n",
    "            fp=Path(args.rag_dir)/f'{u.name}.faiss'; cp=Path(args.rag_dir)/f'{u.name}.chunks.json'\n",
    "            if fp.exists() and cp.exists(): rag=RAG(fp, cp)\n",
    "            ad=str(Path(args.adapters_dir)/u.name/'lora_adapter') if args.mode=='llm_full' else None\n",
    "            sg=LLM(args.base_model, adapter_dir=ad, rag=rag, lex=lex, bf16=args.bf16)\n",
    "        res=sim(load_text(t), sg, k=args.k)\n",
    "        out.append({'user': u.name, 'mode': args.mode, **res}); print(u.name, args.mode, f\"KSS={res['kss']:.3f}\")\n",
    "    Path(args.results_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(args.results_csv,'w',newline='',encoding='utf-8') as f:\n",
    "        w=csv.DictWriter(f, fieldnames=list(out[0].keys()) if out else ['user','mode','kss'])\n",
    "        w.writeheader(); [w.writerow(r) for r in out]\n",
    "    print('csv →', args.results_csv)\n",
    "if __name__=='__main__': main()\n",
    "'''))\n",
    "print('code rooted at', CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data → authors.jsonl (robust download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content\n",
    "F=enron_mail_20150507.tgz\n",
    "rm -f \"$F\"\n",
    "for URL in \\\n",
    "  \"http://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\" \\\n",
    "  \"http://www.cs.cmu.edu/~enron/enron_mail_20150507.tgz\" \\\n",
    "  \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\" \\\n",
    "  \"https://www.cs.cmu.edu/~enron/enron_mail_20150507.tgz\"; do\n",
    "  echo \"try $URL\"; if curl -fL --retry 5 --retry-all-errors -o \"$F\" \"$URL\"; then break; fi\n",
    "done\n",
    "python - <<'PY'\n",
    "fn='enron_mail_20150507.tgz'\n",
    "with open(fn,'rb') as f:\n",
    "    sig=f.read(2)\n",
    "import sys, pathlib\n",
    "if sig != b'\\x1f\\x8b':\n",
    "    print('not gzip; first 200 bytes:'); print(pathlib.Path(fn).read_bytes()[:200]); sys.exit(2)\n",
    "print('gzip ok')\n",
    "PY\n",
    "tar -xzf \"$F\"\n",
    "python - <<'PY'\n",
    "from pathlib import Path; import sys\n",
    "sys.path.append('/content/drive/MyDrive/assistive_keyboard_7B/code')\n",
    "from src.data.enron_loader import build_authors_jsonl\n",
    "build_authors_jsonl('/content/enron_mail_20150507/maildir', '/content/drive/MyDrive/assistive_keyboard_7B/data/processed/authors.jsonl', min_doc_tokens=20)\n",
    "print('authors.jsonl → /content/drive/MyDrive/assistive_keyboard_7B/data/processed/authors.jsonl')\n",
    "PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splits (author‑disjoint) + per‑user slices (use config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path; import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B')\n",
    "sys.path.append(str(root/'code'))\n",
    "sys.argv = [\n",
    "  'splits',\n",
    "  '--authors_jsonl', str(root/'data/processed/authors.jsonl'),\n",
    "  '--out_dir',       str(root/'splits'),\n",
    "  '--min_tokens',    str(ADAPT_TOKENS+VAL_TOKENS+TEST_TOKENS),\n",
    "  '--adapt_tokens',  str(ADAPT_TOKENS),\n",
    "  '--val_tokens',    str(VAL_TOKENS),\n",
    "  '--test_tokens',   str(TEST_TOKENS),\n",
    "  '--max_test_authors', str(MAX_TEST_AUTHORS),\n",
    "  '--seed',          str(SEED)\n",
    "]\n",
    "from src.splits.make_splits import main as run; run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prefetch base model (warms HF cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if USE_BF16_INSTEAD_OF_4BIT:\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=getattr(__import__('torch'),'bfloat16'))\n",
    "else:\n",
    "    m = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', load_in_4bit=True)\n",
    "del m\n",
    "print('prefetch ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per‑user assets: lexicon + FAISS RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "sys.argv = ['lex','--users_dir', str(root/'users'), '--out_dir', str(root/'lexicons'), '--max_items','4000']\n",
    "from src.lexicon.build_lexicon import main as lex; lex()\n",
    "sys.argv = ['rag','--users_dir', str(root/'users'), '--out_dir', str(root/'rag'), '--model_name','sentence-transformers/all-MiniLM-L6-v2', '--chunk_chars','300','--overlap','50']\n",
    "from src.rag.build_rag import main as rag; rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA per user (resumable ckpts; bf16 toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "\n",
    "# hot‑patch loader to flip 4bit ↔ bf16 without editing files\n",
    "import src.lora.train_lora as TL\n",
    "orig = TL.AutoModelForCausalLM.from_pretrained\n",
    "def patched(*a, **kw):\n",
    "    kw = dict(kw)\n",
    "    if USE_BF16_INSTEAD_OF_4BIT:\n",
    "        kw.pop('load_in_4bit', None)\n",
    "        kw['torch_dtype'] = getattr(__import__('torch'), 'bfloat16')\n",
    "    else:\n",
    "        kw['load_in_4bit'] = True\n",
    "        kw['torch_dtype'] = getattr(__import__('torch'), 'float16')\n",
    "    return orig(*a, **kw)\n",
    "TL.AutoModelForCausalLM.from_pretrained = patched\n",
    "\n",
    "sys.argv = [\n",
    "  'train',\n",
    "  '--users_dir',    str(root/'users'),\n",
    "  '--adapters_dir', str(root/'adapters'),\n",
    "  '--base_model',   BASE_MODEL,\n",
    "  '--rank',         str(LORA_RANK),\n",
    "  '--alpha',        str(LORA_ALPHA),\n",
    "  '--dropout',      str(LORA_DROPOUT),\n",
    "  '--lr',           '2e-4',\n",
    "  '--steps',        str(LORA_STEPS),\n",
    "  '--block_size',   '256'\n",
    "]\n",
    "from src.lora.train_lora import main as train; train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval (ngram, llm_base, llm_lex, llm_full) → CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "def run_eval(mode, name):\n",
    "    sys.argv = [\n",
    "      'eval',\n",
    "      '--users_dir',   str(root/'users'),\n",
    "      '--mode',        mode,\n",
    "      '--base_model',  BASE_MODEL,\n",
    "      '--adapters_dir',str(root/'adapters'),\n",
    "      '--lexicons_dir',str(root/'lexicons'),\n",
    "      '--rag_dir',     str(root/'rag'),\n",
    "      '--results_csv', str(root/'runs'/f'leaderboard_{name}.csv'),\n",
    "    ] + (['--bf16'] if USE_BF16_INSTEAD_OF_4BIT else [])\n",
    "    from src.eval.typing_sim import main as E; E()\n",
    "for mode,name in [('ngram','ngram'),('llm_base','llm_base'),('llm_lex','llm_lex'),('llm_full','llm_full')]:\n",
    "    run_eval(mode,name)\n",
    "print('eval ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results (means + per‑author spread + outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "root='/content/drive/MyDrive/assistive_keyboard_7B/runs'\n",
    "dfs=[]\n",
    "for f in ['leaderboard_ngram.csv','leaderboard_llm_base.csv','leaderboard_llm_lex.csv','leaderboard_llm_full.csv']:\n",
    "    p=os.path.join(root,f)\n",
    "    try: dfs.append(pd.read_csv(p).assign(model=f.replace('leaderboard_','').replace('.csv','')))\n",
    "    except Exception as e: print('missing', p, e)\n",
    "res=pd.concat(dfs, ignore_index=True)\n",
    "print('== means by model ==')\n",
    "print(res.groupby('model')[['kss','time_ms','accepts']].mean().round(3))\n",
    "full=res[res['model']=='llm_full'][['user','kss']]\n",
    "print('\\n== per-author KSS (llm_full) describe ==')\n",
    "print(full.describe().round(3))\n",
    "med=full['kss'].median(); mad=(full['kss']-med).abs().median()\n",
    "bad=full[full['kss']<med-1.5*mad]['user'].tolist()\n",
    "print('\\noutliers (low KSS):', bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick live check (single author suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "root=Path('/content/drive/MyDrive/assistive_keyboard_7B'); sys.path.append(str(root/'code'))\n",
    "from src.infer.suggest import LLM, Lex, RAG\n",
    "from transformers import AutoTokenizer\n",
    "authors=[p.name for p in (root/'users').iterdir() if p.is_dir()]\n",
    "author=authors[0] if authors else None\n",
    "print('author:', author)\n",
    "lex=None; lpath=root/'lexicons'/f'{author}.lexicon.json'\n",
    "if lpath.exists():\n",
    "    lex=Lex(AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True), lpath.read_text(encoding='utf-8'), cap=2.5)\n",
    "rag=None; f=root/'rag'/f'{author}.faiss'; c=root/'rag'/f'{author}.chunks.json'\n",
    "if f.exists() and c.exists():\n",
    "    rag=RAG(f, c)\n",
    "adapter=str(root/'adapters'/author/'lora_adapter')\n",
    "sg=LLM(BASE_MODEL, adapter_dir=adapter, rag=rag, lex=lex, bf16=USE_BF16_INSTEAD_OF_4BIT)\n",
    "ctx='Hi team, following up on the budget approval for Q4. If we can align by Friday,'\n",
    "print('ctx:', ctx)\n",
    "print('suggestions:', sg.suggest(ctx, k=3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
